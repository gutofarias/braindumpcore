:PROPERTIES:
:ID:       48f41ab0-c757-4e91-97b9-62b8ee8914e3
:END:
#+title: Playing Atari with Deep RL

* Playing Atari With Deep RL cite:mnih2013playing

** Preprocessing Steps

1. Obtain raw pixels of size $210 \times 160$
2. Grayscale and downsample to $110 \times 84$
3. Crop representative $84 \times 84$ region
4. Stack the last 4 frames in history to form the $84 \times 84 \times
   4$ input

** DQN

1. Use of [[id:819e7988-cd1b-42c3-a714-d761aeea0134][Experience Replay]] buffer
2. Separate target network stabilizes optimization targets:

\begin{equation}
  \delta = r_t + \gamma \mathrm{max}_a Q(s_{t+1}, a ; \theta') -
  Q(s_t, a_t; \theta)
\end{equation}

The network parameterized with $\theta '$ is a snapshot of the network
at some point in time, so the optimization target doesn't change so
rapidly.

3. Clip $\delta$ to $\left[1, -1\right]$

* Improving DQN
- Double Q-learning reduces bias cite:van2016deep
- Average Q-learning reduces variance cite:anschel2017averaged
- [[id:33b0e88f-317f-4fa1-b239-63ebbc04fa58][Hindsight Experience Replay]] cite:andrychowicz2017hindsight
- Distributional RL cite:dabney2018distributional

* References
- [[https://davidsanwald.github.io/2016/12/11/Double-DQN-interfacing-OpenAi-Gym.html][Defeating the Deadly Triad: | random walks and lots of â™¥s]]

bibliography:biblio.bib
