:PROPERTIES:
:ID:       871b5f62-1c26-4572-aca9-9a27c6b102f1
:END:
#+title: Gibbs' Inequality

The relative entropy or [[id:3cf8b996-d512-4791-8db1-9a0f8de928ef][kl divergence]] between two probability
distributions $P(x)$ and $Q(x)$ defined over the same alphabet
$\mathcal{A}_X$ is:

\begin{equation}
  D_{\textrm{KL}}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
\end{equation}

Gibbs Inequality states that:

\begin{equation}
  D_{\textrm{KL}}(P||Q) \ge 0
\end{equation}

for any $P$ and $Q$.
