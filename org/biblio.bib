
@article{10.3389/fnbot.2019.00018,
  title = {Supervised Learning in {{SNN}} via Reward-Modulated Spike-Timing-Dependent Plasticity for a Target Reaching Vehicle},
  author = {Bing, Zhenshan and Baumann, Ivan and Jiang, Zhuangyi and Huang, Kai and Cai, Caixia and Knoll, Alois},
  date = {2019},
  journaltitle = {Frontiers in Neurorobotics},
  volume = {13},
  pages = {18},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2019.00018},
  url = {https://www.frontiersin.org/article/10.3389/fnbot.2019.00018},
  abstract = {Spiking neural networks (SNNs) offer many advantages over traditional artificial neural networks (ANNs) such as biological plausibility, fast information processing, and energy efficiency. Although SNNs have been used to solve a variety of control tasks using the Spike-Timing-Dependent Plasticity (STDP) learning rule, existing solutions usually involve hard-coded network architectures solving specific tasks rather than solving different kinds of tasks generally. This results in neglecting one of the biggest advantages of ANNs, i.e., being general-purpose and easy-to-use due to their simple network architecture, which usually consists of an input layer, one or multiple hidden layers and an output layer. This paper addresses the problem by introducing an end-to-end learning approach of spiking neural networks constructed with one hidden layer and reward-modulated Spike-Timing-Dependent Plasticity (R-STDP) synapses in an all-to-all fashion. We use the supervised reward-modulated Spike-Timing-Dependent-Plasticity learning rule to train two different SNN-based sub-controllers to replicate a desired obstacle avoiding and goal approaching behavior, provided by pre-generated datasets. Together they make up a target-reaching controller, which is used to control a simulated mobile robot to reach a target area while avoiding obstacles in its path. We demonstrate the performance and effectiveness of our trained SNNs to achieve target reaching tasks in different unknown scenarios.}
}

@article{10.3389/fncom.2017.00024,
  title = {Equilibrium Propagation: {{Bridging}} the Gap between Energy-Based Models and Backpropagation},
  author = {Scellier, Benjamin and Bengio, Yoshua},
  date = {2017},
  journaltitle = {Frontiers in Computational Neuroscience},
  volume = {11},
  pages = {24},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00024},
  url = {https://www.frontiersin.org/article/10.3389/fncom.2017.00024},
  abstract = {We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well-defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point or stationary distribution) toward a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged toward their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal “back-propagated” during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not. We also show experimentally that multi-layer recurrently connected networks with 1, 2, and 3 hidden layers can be trained by Equilibrium Propagation on the permutation-invariant MNIST task.}
}

@article{10.3389/fninf.2018.00089,
  title = {{{BindsNET}}: {{A}} Machine Learning-Oriented Spiking Neural Networks Library in Python},
  author = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann, Hava T. and Kozma, Robert},
  date = {2018},
  journaltitle = {Frontiers in Neuroinformatics},
  volume = {12},
  pages = {89},
  issn = {1662-5196},
  doi = {10.3389/fninf.2018.00089},
  url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00089}
}

@article{10.3389/fnins.2015.00481,
  title = {Poker-Dvs and {{MNIST}}-{{DVS}}. {{Their}} History, How They Were Made, and Other Details},
  author = {Serrano-Gotarredona, Teresa and Linares-Barranco, Bernabé},
  date = {2015},
  journaltitle = {Frontiers in Neuroscience},
  volume = {9},
  pages = {481},
  issn = {1662-453X},
  doi = {10.3389/fnins.2015.00481},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2015.00481},
  abstract = {This article reports on two databases for event-driven object recognition using a Dynamic Vision Sensor (DVS). The first, which we call Poker-DVS and is being released together with this article, was obtained by browsing specially made poker card decks in front of a DVS camera for 2–4 s. Each card appeared on the screen for about 20–30 ms. The poker pips were tracked and isolated off-line to constitute the 131-recording Poker-DVS database. The second database, which we call MNIST-DVS and which was released in December 2013, consists of a set of 30,000 DVS camera recordings obtained by displaying 10,000 moving symbols from the standard MNIST 70,000-picture database on an LCD monitor for about 2–3 s each. Each of the 10,000 symbols was displayed at three different scales, so that event-driven object recognition algorithms could easily be tested for different object sizes. This article tells the story behind both databases, covering, among other aspects, details of how they work and the reasons for their creation. We provide not only the databases with corresponding scripts, but also the scripts and data used to generate the figures shown in this article (as Supplementary Material).}
}

@article{10.3389/fnins.2016.00508,
  title = {Training Deep Spiking Neural Networks Using Backpropagation},
  author = {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  date = {2016},
  journaltitle = {Frontiers in Neuroscience},
  volume = {10},
  pages = {508},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00508},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2016.00508},
  abstract = {Deep spiking neural networks (SNNs) hold the potential for improving the latency and energy efficiency of deep neural networks through data-driven event-based computation. However, training such networks is difficult due to the non-differentiable nature of spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are considered as noise. This enables an error backpropagation mechanism for deep SNNs that follows the same principles as in conventional deep networks, but works directly on spike signals and membrane potentials. Compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statistics of spikes more precisely. We evaluate the proposed framework on artificially generated events from the original MNIST handwritten digit benchmark, and also on the N-MNIST benchmark recorded with an event-based dynamic vision sensor, in which the proposed method reduces the error rate by a factor of more than three compared to the best previous SNN, and also achieves a higher accuracy than a conventional convolutional neural network (CNN) trained and tested on the same data. We demonstrate in the context of the MNIST task that thanks to their event-driven operation, deep SNNs (both fully connected and convolutional) trained with our method achieve accuracy equivalent with conventional neural networks. In the N-MNIST example, equivalent accuracy is achieved with about five times fewer computational operations.},
  file = {/home/jethro/Zotero/storage/EUP5P3XN/Lee et al. - 2016 - Training deep spiking neural networks using backpr.pdf}
}

@article{10.3389/fnins.2018.00435,
  title = {Training Deep Spiking Convolutional Neural Networks with {{STDP}}-{{Based}} Unsupervised Pre-Training Followed by Supervised Fine-Tuning},
  author = {Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  date = {2018},
  journaltitle = {Frontiers in Neuroscience},
  volume = {12},
  pages = {435},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00435},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2018.00435},
  abstract = {Spiking Neural Networks (SNNs) are fast becoming a promising candidate for brain-inspired neuromorphic computing because of their inherent power efficiency and impressive inference accuracy across several cognitive tasks such as image classification and speech recognition. The recent efforts in SNNs have been focused on implementing deeper networks with multiple hidden layers to incorporate exponentially more difficult functional representations. In this paper, we propose a pre-training scheme using biologically plausible unsupervised learning, namely Spike-Timing-Dependent-Plasticity (STDP), in order to better initialize the parameters in multi-layer systems prior to supervised optimization. The multi-layer SNN is comprised of alternating convolutional and pooling layers followed by fully-connected layers, which are populated with leaky integrate-and-fire spiking neurons. We train the deep SNNs in two phases wherein, first, convolutional kernels are pre-trained in a layer-wise manner with unsupervised learning followed by fine-tuning the synaptic weights with spike-based supervised gradient descent backpropagation. Our experiments on digit recognition demonstrate that the STDP-based pre-training with gradient-based optimization provides improved robustness, faster ( 2.5 ×) training time and better generalization compared with purely gradient-based training without pre-training.}
}

@inproceedings{abbeel2004apprenticeship,
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  booktitle = {Proceedings of the Twenty-First International Conference on {{Machine}} Learning},
  author = {Abbeel, Pieter and Ng, Andrew Y},
  date = {2004},
  pages = {1},
  organization = {{ACM}}
}

@article{aenugu19_reinf_learn_with_spikin_coagen,
  title = {Reinforcement Learning with Spiking Coagents},
  author = {Aenugu, Sneha and Sharma, Abhishek and Yelamarthi, Sasikiran and Hazan, Hananel and Thomas, Philip S. and Kozma, Robert},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1910.06489v2},
  abstract = {Neuroscientific theory suggests that dopaminergic neurons broadcast global reward prediction errors to large areas of the brain influencing the synaptic plasticity of the neurons in those regions. We build on this theory to propose a multi-agent learning framework with spiking neurons in the generalized linear model (GLM) formulation as agents, to solve reinforcement learning (RL) tasks. We show that a network of GLM spiking agents connected in a hierarchical fashion, where each spiking agent modulates its firing policy based on local information and a global prediction error, can learn complex action representations to solve RL tasks. We further show how leveraging principles of modularity and population coding inspired from the brain can help reduce variance in the learning updates making it a viable optimization technique.},
  archivePrefix = {arXiv},
  eprint = {1910.06489},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{aiskinLee,
  title = {A Neuro-Inspired Artificial Peripheral Nervous System for Scalable Electronic Skins},
  author = {Lee, Wang Wei and Tan, Yu Jun and Yao, Haicheng and Li, Si and See, Hian Hian and Hon, Matthew and Ng, Kian Ann and Xiong, Betty and Ho, John S. and Tee, Benjamin C. K.},
  date = {2019},
  journaltitle = {Science Robotics},
  volume = {4},
  publisher = {{Science Robotics}},
  doi = {10.1126/scirobotics.aax2198},
  url = {https://robotics.sciencemag.org/content/4/32/eaax2198},
  abstract = {The human sense of touch is essential for dexterous tool usage, spatial awareness, and social communication. Equipping intelligent human-like androids and prosthetics with electronic skins—a large array of sensors spatially distributed and capable of rapid somatosensory perception—will enable them to work collaboratively and naturally with humans to manipulate objects in unstructured living environments. Previously reported tactile-sensitive electronic skins largely transmit the tactile information from sensors serially, resulting in readout latency bottlenecks and complex wiring as the number of sensors increases. Here, we introduce the Asynchronously Coded Electronic Skin (ACES)—a neuromimetic architecture that enables simultaneous transmission of thermotactile information while maintaining exceptionally low readout latencies, even with array sizes beyond 10,000 sensors. We demonstrate prototype arrays of up to 240 artificial mechanoreceptors that transmitted events asynchronously at a constant latency of 1 ms while maintaining an ultra-high temporal precision of \&lt;60 ns, thus resolving fine spatiotemporal features necessary for rapid tactile perception. Our platform requires only a single electrical conductor for signal propagation, realizing sensor arrays that are dynamically reconfigurable and robust to damage. We anticipate that the ACES platform can be integrated with a wide range of skin-like sensors for artificial intelligence (AI)–enhanced autonomous robots, neuroprosthetics, and neuromorphic computing hardware for dexterous object manipulation and somatosensory perception.},
  elocation-id = {eaax2198},
  eprint = {https://robotics.sciencemag.org/content/4/32/eaax2198.full.pdf},
  number = {32}
}

@inproceedings{alonsoCurrentResearchTrends2019,
  title = {Current {{Research Trends}} in {{Robot Grasping}} and {{Bin Picking}}},
  booktitle = {International {{Joint Conference SOCO}}’18-{{CISIS}}’18-{{ICEUTE}}’18},
  author = {Alonso, Marcos and Izaguirre, Alberto and Graña, Manuel},
  editor = {Graña, Manuel and López-Guede, José Manuel and Etxaniz, Oier and Herrero, Álvaro and Sáez, José Antonio and Quintián, Héctor and Corchado, Emilio},
  date = {2019},
  pages = {367--376},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-94120-2_35},
  abstract = {We provide a view of current research issues in Robotic Grasping and Bin Picking focused on the perception aspects of the problem, mainly related to computer vision algorithms. After recalling the evolution of the topics in the last decades, we focus on the modern use of Deep Learning Algorithms. Two main trends are followed in the approaches to innovative grasping techniques. First, Convolutional Neural Networks are used for grasping perceptual aspects. We discuss the different degrees of success of several published approaches. Second, Deep Reinforcement Learning is being extensively tested in order to develop integrated eye-hand coordination systems not requiring delicate calibration. We provide also a discussion of possible future lines of research.},
  file = {/home/jethro/Zotero/storage/3DYE53G7/Alonso et al. - 2019 - Current Research Trends in Robot Grasping and Bin .pdf},
  isbn = {978-3-319-94120-2},
  keywords = {Bin Picking,Convolutional Neural Networks,Deep Learning,Robot grasping,Robot gripper,Robot planning},
  langid = {english},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}}
}

@inproceedings{andrychowicz2017hindsight,
  title = {Hindsight Experience Replay},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  date = {2017},
  pages = {5048--5058}
}

@inproceedings{anschel2017averaged,
  title = {Averaged-Dqn: {{Variance}} Reduction and Stabilization for Deep Reinforcement Learning},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  author = {Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  date = {2017},
  pages = {176--185},
  organization = {{JMLR. org}}
}

@article{arjovsky19_invar_risk_minim,
  title = {Invariant Risk Minimization},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1907.02893v1},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archivePrefix = {arXiv},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  primaryClass = {stat.ML}
}

@article{baltrusaitis17:_multim_machin_learn,
  title = {Multimodal Machine Learning: {{A}} Survey and Taxonomy},
  author = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1705.09406v2},
  abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. Baltru, Tadas, Ahuja, C., \& Morency, L., Multimodal machine learning: a survey and taxonomy, CoRR, (), (2017). In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
  archivePrefix = {arXiv},
  eprint = {1705.09406},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@online{bartekBartekCodingBlog2017,
  title = {Bartek's Coding Blog: {{How}} Not\_null Can Improve Your Code?},
  author = {Bartek},
  date = {2017-10-16},
  journaltitle = {How not\_null can improve your code?},
  url = {https://www.bfilipek.com/2017/10/notnull.html},
  urldate = {2020-06-24},
  file = {/home/jethro/Zotero/storage/JHGE78LF/notnull.html}
}

@article{bellec18_long_short_term_memor_learn,
  title = {Long Short-Term Memory and Learning-to-Learn in Networks of Spiking Neurons},
  author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1803.09574v4},
  abstract = {Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with artificial neural networks (ANNs). We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1803.09574},
  eprinttype = {arxiv},
  primaryClass = {cs.NE}
}

@book{bishop2006pattern,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M},
  date = {2006},
  publisher = {{springer}}
}

@online{blouwBenchmarkingKeywordSpotting2019,
  title = {Benchmarking {{Keyword Spotting Efficiency}} on {{Neuromorphic Hardware}}},
  author = {Blouw, Peter and Choo, Xuan and Hunsberger, Eric and Eliasmith, Chris},
  date = {2019-04-02},
  url = {http://arxiv.org/abs/1812.01739},
  urldate = {2020-06-03},
  abstract = {Using Intel's Loihi neuromorphic research chip and ABR's Nengo Deep Learning toolkit, we analyze the inference speed, dynamic power consumption, and energy cost per inference of a two-layer neural network keyword spotter trained to recognize a single phrase. We perform comparative analyses of this keyword spotter running on more conventional hardware devices including a CPU, a GPU, Nvidia's Jetson TX1, and the Movidius Neural Compute Stick. Our results indicate that for this inference application, Loihi outperforms all of these alternatives on an energy cost per inference basis while maintaining equivalent inference accuracy. Furthermore, an analysis of tradeoffs between network size, inference speed, and energy cost indicates that Loihi's comparative advantage over other low-power computing devices improves for larger networks.},
  archivePrefix = {arXiv},
  eprint = {1812.01739},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/2DLG3N7A/Blouw et al. - 2019 - Benchmarking Keyword Spotting Efficiency on Neurom.pdf;/home/jethro/Zotero/storage/54UJ72RY/1812.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Updated power measurements on Loihi after improvements to I/O speed. Updated discussion in light of results},
  primaryClass = {cs, stat}
}

@article{borman2004expectation,
  title = {The Expectation Maximization Algorithm-a Short Tutorial},
  author = {Borman, Sean},
  date = {2004},
  journaltitle = {Submitted for publication},
  volume = {41}
}

@article{brown18_machin_teach_inver_reinf_learn,
  title = {Machine Teaching for Inverse Reinforcement Learning: {{Algorithms}} and Applications},
  author = {Brown, Daniel S. and Niekum, Scott},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1805.07687v6},
  abstract = {Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decision-making task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximally-informative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.},
  archivePrefix = {arXiv},
  eprint = {1805.07687},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{buckar_c_packag_manag_missin,
  title = {Buckaroo - the {{C}}++ Package Manager Valueₚtr - the Missing {{C}}++ Smart-Pointer},
  author = {{Buckaroo}},
  date = {2019},
  url = {https://buckaroo.pm/posts/value-ptr-the-missing-smart-ptr/},
  note = {Online; accessed 24 February 2019}
}

@article{camerer2004cognitive,
  title = {A Cognitive Hierarchy Model of Games},
  author = {Camerer, Colin F and Ho, Teck-Hua and Chong, Juin-Kuan},
  date = {2004},
  journaltitle = {The Quarterly Journal of Economics},
  volume = {119},
  pages = {861--898},
  publisher = {{MIT Press}},
  number = {3}
}

@online{CAUSALITYDiscussion,
  title = {{{CAUSALITY}} - {{Discussion}}},
  url = {http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html},
  urldate = {2020-06-24},
  file = {/home/jethro/Zotero/storage/U245TCDC/d-sep.html}
}

@book{chatterjee06_regres_analy_examp,
  title = {Regression Analysis by Example},
  author = {Chatterjee, Samprit and Hadi, Ali S.},
  date = {2006},
  publisher = {{John Wiley \& Sons, Inc.}},
  doi = {10.1002/0470055464},
  url = {https://doi.org/10.1002/0470055464},
  date_added = {Tue Jan 15 13:13:16 2019},
  pagetotal = {nil},
  series = {Wiley Series in Probability and Statistics}
}

@article{chen16_xgboos,
  title = {Xgboost: A Scalable Tree Boosting System},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1603.02754v3},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archivePrefix = {arXiv},
  eprint = {1603.02754},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{chen18_neural_ordin_differ_equat,
  title = {Neural Ordinary Differential Equations},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1806.07366v4},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{chen20_simpl_framew_contr_learn_visual_repres,
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/2002.05709v1},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5 \% top-1 accuracy, which is a 7 \% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1 \% of the labels, we achieve 85.8 \% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archivePrefix = {arXiv},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/IWQD33L4/Chen et al. - 2020 - A simple framework for contrastive learning of vis.pdf},
  primaryClass = {cs.LG}
}

@article{chen3rabit,
  title = {{{RABIT}}: {{A}} Reliable Allreduce and Broadcast Interface},
  author = {Chen, Tianqi and Cano, Ignacio and Zhou, Tianyi},
  journaltitle = {Transfer},
  volume = {3},
  number = {2}
}

@online{chenBigSelfSupervisedModels2020,
  title = {Big {{Self}}-{{Supervised Models}} Are {{Strong Semi}}-{{Supervised Learners}}},
  author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-06-17},
  url = {http://arxiv.org/abs/2006.10029},
  urldate = {2020-07-08},
  abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\textbackslash\% ImageNet top-1 accuracy with just 1\textbackslash\% of the labels (\$\textbackslash le\$13 labeled images per class) using ResNet-50, a \$10\textbackslash times\$ improvement in label efficiency over the previous state-of-the-art. With 10\textbackslash\% of labels, ResNet-50 trained with our method achieves 77.5\textbackslash\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  archivePrefix = {arXiv},
  eprint = {2006.10029},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/E2QRPVJ6/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf;/home/jethro/Zotero/storage/UZC88A3I/2006.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: code and pretrained models at https://github.com/google-research/simclr},
  primaryClass = {cs, stat}
}

@software{codesbayCodesBayCplusPlusSmartPointer2020,
  title = {{{CodesBay}}/{{CplusPlus}}\_{{SmartPointer}}},
  author = {CodesBay},
  date = {2020-06-08},
  origdate = {2016-05-25T09:58:35Z},
  url = {https://github.com/CodesBay/CplusPlus_SmartPointer},
  urldate = {2020-06-24},
  abstract = {This repository contains description of C++11 and C++14 Smart Pointers Trilogy of shared\_ptr, unique\_ptr and weak\_ptr}
}

@article{colyer_tmp_neural_ode,
  title = {Neural Ordinary Differential Equations},
  author = {Colyer, Adrian},
  date = {2019},
  url = {https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/},
  note = {Online; accessed 28 February 2019}
}

@article{comsa19_tempor_codin_spikin_neural_networ,
  title = {Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function},
  author = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala, Jyrki},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1907.13223v1},
  abstract = {The timing of individual neuronal spikes is essential for biological brains to make fast responses to sensory stimuli. However, conventional artificial neural networks lack the intrinsic temporal coding ability present in biological networks. We propose a spiking neural network model that encodes information in the relative timing of individual neuron spikes. In classification tasks, the output of the network is indicated by the first neuron to spike in the output layer. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. The network operates using a biologically-plausible alpha synaptic transfer function. Additionally, we use trainable synchronisation pulses that provide bias, add flexibility during training and exploit the decay part of the alpha function. We show that such networks can be trained successfully on noisy Boolean logic tasks and on the MNIST dataset encoded in time. The results show that the spiking neural network outperforms comparable spiking models on MNIST and achieves similar quality to fully connected conventional networks with the same architecture. We also find that the spiking network spontaneously discovers two operating regimes, mirroring the accuracy-speed trade-off observed in human decision-making: a slow regime, where a decision is taken after all hidden neurons have spiked and the accuracy is very high, and a fast regime, where a decision is taken very fast but the accuracy is lower. These results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. By studying temporal coding in spiking networks, we aim to create building blocks towards energy-efficient and more complex biologically-inspired neural architectures.},
  archivePrefix = {arXiv},
  eprint = {1907.13223},
  eprinttype = {arxiv},
  primaryClass = {cs.NE}
}

@article{Cybenko1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12-01},
  journaltitle = {Mathematics of Control, Signals and Systems},
  volume = {2},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  url = {https://doi.org/10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  number = {4}
}

@inproceedings{dabney2018distributional,
  title = {Distributional Reinforcement Learning with Quantile Regression},
  booktitle = {Thirty-Second {{AAAI}} Conference on Artificial Intelligence},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, Rémi},
  date = {2018}
}

@article{dacrema19_are_we_reall_makin_much_progr,
  title = {Are We Really Making Much Progress? {{A}} Worrying Analysis of Recent Neural Recommendation Approaches},
  author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1907.06902v1},
  abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019\textsubscript{D}eepLearning\textsubscript{E}valuation.},
  archivePrefix = {arXiv},
  eprint = {1907.06902v1},
  eprinttype = {arxiv},
  primaryClass = {cs.IR}
}

@article{dan_reprod,
  title = {Reproducible Research: {{Stripe}}'s Approach to Data Science},
  author = {Frank, Dan},
  date = {2016},
  url = {https://stripe.com/blog/reproducible-research},
  note = {Online; accessed 06 January 2019}
}

@inproceedings{dann2017unifying,
  title = {Unifying {{PAC}} and Regret: {{Uniform PAC}} Bounds for Episodic Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  date = {2017},
  pages = {5713--5723}
}

@article{davies2018loihi,
  ids = {davies\_loihi\_2018},
  title = {Loihi: {{A}} Neuromorphic Manycore Processor with on-Chip Learning},
  author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and others},
  date = {2018},
  journaltitle = {IEEE Micro},
  volume = {38},
  pages = {82--99},
  publisher = {{IEEE}},
  eventtitle = {{{IEEE Micro}}},
  file = {/home/jethro/Zotero/storage/K68GKRLW/davies2018.pdf;/home/jethro/Zotero/storage/W945BY83/8259423.html},
  keywords = {Algorithm design and analysis,artificial intelligence,Biological neural networks,circuit optimisation,Computational modeling,Computer architecture,CPU iso-process-voltage-area,dendritic compartments,hierarchical connectivity,integrated circuit modelling,Intels process,LASSO optimization problems,learning (artificial intelligence),locally competitive algorithm,Loihi,machine learning,magnitude superior energy-delay-product,microprocessor chips,multiprocessing systems,neural chips,neuromorphic computing,neuromorphic manycore processor,Neuromorphics,Neurons,on-chip learning,programmable synaptic learning rules,size 14 nm,spike-based computation,spiking neural networks,synaptic delays},
  number = {1}
}

@book{DBLP:books/oreilly/Kleppmann2014,
  title = {Designing Data-Intensive Applications: {{The}} Big Ideas behind Reliable, Scalable, and Maintainable Systems},
  author = {Kleppmann, Martin},
  date = {2016},
  publisher = {{O'Reilly}},
  url = {http://shop.oreilly.com/product/0636920032175.do},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/books/oreilly/Kleppmann2014},
  isbn = {978-1-4493-7332-0},
  timestamp = {Mon, 27 May 2019 17:50:46 +0200}
}

@book{deisenrothMathematicsMachineLearning2020,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
  date = {2020-02-29},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108679930},
  url = {https://www.cambridge.org/core/product/identifier/9781108679930/type/book},
  urldate = {2020-06-22},
  file = {/home/jethro/Zotero/storage/EZL8DHUY/Deisenroth et al. - 2020 - Mathematics for Machine Learning.pdf},
  isbn = {978-1-108-67993-0 978-1-108-47004-9 978-1-108-45514-5},
  langid = {english}
}

@inproceedings{dremel,
  title = {Dremel: {{Interactive}} Analysis of Web-Scale Datasets},
  booktitle = {Proc. of the 36th Int'l Conf on Very Large Data Bases},
  author = {Melnik, Sergey and Gubarev, Andrey and Long, Jing Jing and Romer, Geoffrey and Shivakumar, Shiva and Tolton, Matt and Vassilakis, Theo},
  date = {2010},
  pages = {330--339},
  url = {http://www.vldb2010.org/accept.htm}
}

@article{drepper2007every,
  title = {What Every Programmer Should Know about Memory},
  author = {Drepper, Ulrich},
  date = {2007}
}

@inproceedings{Du2010APA,
  title = {A {{POMDP}} Approach to Robot Motion Planning under Uncertainty},
  author = {Du, Yanzhu and Hsu, David and Kurniawati, Hanna and Lee, Wee Sun and Ong, Sylvie C. W. and Png, Shao Wei},
  date = {2010}
}

@article{dupont19_augmen_neural_odes,
  title = {Augmented Neural Odes},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1904.01681v1},
  abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
  archivePrefix = {arXiv},
  eprint = {1904.01681},
  eprinttype = {arxiv},
  primaryClass = {stat.ML}
}

@article{espeholt18_impal,
  title = {Impala: {{Scalable}} Distributed Deep-Rl with Importance Weighted Actor-Learner Architectures},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1802.01561v3},
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  archivePrefix = {arXiv},
  eprint = {1802.01561},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{eysenbach18_diver_is_all_you_need,
  title = {Diversity Is All You Need: {{Learning}} Skills without a Reward Function},
  author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1802.06070v6},
  abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1802.06070},
  eprinttype = {arxiv},
  primaryClass = {cs.AI}
}

@article{felipe_demys_join_algor,
  title = {Demystifying {{JOIN}} Algorithms},
  author = {Carvalho, Felipe Oliveira},
  date = {2019},
  url = {http://blog.felipe.rs/2019/01/29/demystifying-join-algorithms/},
  note = {Online; accessed 03 February 2019}
}

@online{filipekHowNotNull,
  ids = {filipek\_how\_nodate-1},
  title = {How Not\_null Can Improve Your Code?},
  author = {Filipek, Bartlomiej},
  url = {https://www.bfilipek.com/2017/10/notnull.html},
  urldate = {2020-06-24},
  abstract = {Can not\_null from Core Guidelines help us and simplify code? Let's see a few examples.},
  file = {/home/jethro/Zotero/storage/BBBAUYC4/notnull.html;/home/jethro/Zotero/storage/HE2G6VHR/notnull.html},
  langid = {english}
}

@article{florian07_reinf_learn_throug_modul_spike,
  title = {Reinforcement Learning through Modulation of Spike-Timing-Dependent Synaptic Plasticity},
  author = {Florian, Răzvan V.},
  date = {2007},
  journaltitle = {Neural Computation},
  volume = {19},
  pages = {1468--1502},
  doi = {10.1162/neco.2007.19.6.1468},
  url = {https://doi.org/10.1162/neco.2007.19.6.1468},
  date_added = {Mon Nov 4 15:30:29 2019},
  number = {6}
}

@inproceedings{florian2005,
  title = {A Reinforcement Learning Algorithm for Spiking Neural Networks},
  author = {Florian, Răzvan},
  date = {2005-10},
  volume = {2005},
  pages = {8 pp.-},
  doi = {10.1109/SYNASC.2005.13},
  isbn = {0-7695-2453-2},
  series = {Proceedings - {{Seventh International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}}, {{SYNASC}} 2005}
}

@online{gallegoEventbasedVisionSurvey2020,
  ids = {gallego\_event-based\_2020-1},
  title = {Event-Based {{Vision}}: {{A Survey}}},
  shorttitle = {Event-Based {{Vision}}},
  author = {Gallego, Guillermo and Delbruck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew and Conradt, Joerg and Daniilidis, Kostas and Scaramuzza, Davide},
  date = {2020-02-26},
  url = {http://arxiv.org/abs/1904.08405},
  urldate = {2020-06-25},
  abstract = {Event cameras are bio-inspired sensors that work radically different from traditional cameras. Instead of capturing images at a fixed rate, they measure per-pixel brightness changes asynchronously. This results in a stream of events, which encode the time, location and sign of the brightness changes. Event cameras posses outstanding properties compared to traditional cameras: very high dynamic range (140 dB vs. 60 dB), high temporal resolution (in the order of microseconds), low power consumption, and do not suffer from motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as high speed and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
  archivePrefix = {arXiv},
  eprint = {1904.08405},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/ARS3B6WS/Gallego et al. - 2020 - Event-based Vision A Survey.pdf;/home/jethro/Zotero/storage/JLC35NNQ/Gallego et al. - 2020 - Event-based Vision A Survey.pdf;/home/jethro/Zotero/storage/GFHS7GQ4/1904.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@inproceedings{gallegoFocusAllYou2019,
  title = {Focus {{Is All You Need}}: {{Loss Functions}} for {{Event}}-{{Based Vision}}},
  shorttitle = {Focus {{Is All You Need}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gallego, Guillermo and Gehrig, Mathias and Scaramuzza, Davide},
  date = {2019-06},
  pages = {12272--12281},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.01256},
  abstract = {Event cameras are novel vision sensors that output pixel-level brightness changes ("events") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches. We call them focus loss functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/home/jethro/Zotero/storage/34Y4GKWU/Gallego et al. - 2019 - Focus Is All You Need Loss Functions for Event-Ba.pdf;/home/jethro/Zotero/storage/APLG47K4/8954483.html},
  keywords = {asynchronous sensors,cameras,computer vision,computer vision tools,event cameras,Event-based Vision,event-based vision sensors,image resolution,image sensors,image sequences,Low-level Vision,Motion and Tracking,motion compensation,motion compensation methods,optical flow estimation,output pixel-level brightness,shape-from-focus applications,video frames}
}

@article{gary_networ_protoc,
  title = {Network Protocols \&ndash; Programmer's Compendium},
  author = {Bernhardt, Gary},
  date = {2019},
  url = {https://www.destroyallsoftware.com/compendium/network-protocols?shareₖey=97d3ba4c24d21147},
  note = {Online; accessed 25 January 2019}
}

@inproceedings{gehrigEndtoEndLearningRepresentations2019,
  title = {End-to-{{End Learning}} of {{Representations}} for {{Asynchronous Event}}-{{Based Data}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Gehrig, Daniel and Loquercio, Antonio and Derpanis, Konstantinos and Scaramuzza, Davide},
  date = {2019-10},
  pages = {5632--5642},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00573},
  abstract = {Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as "events”. They have appealing advantages over frame based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatio-temporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations by means of strictly differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12\% on optical flow estimation and object recognition over state-of-the-art methods.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  file = {/home/jethro/Zotero/storage/QIGPDJRE/Gehrig et al. - 2019 - End-to-End Learning of Representations for Asynchr.pdf;/home/jethro/Zotero/storage/Z4XURH8V/9009469.html},
  keywords = {asynchronous event-based data,asynchronous streams,Brightness,cameras,Cameras,computer vision,Computer vision,convolutional neural nets,convolutional neural network,end-to-end event representation,end-to-end learning,event cameras,event signal,event streams,frame based cameras,grid-based representation,high dynamic range,high temporal resolution,image motion analysis,image representation,image sensors,image sequences,input event representation,learning (artificial intelligence),motion blur,nonuniform spatiotemporal layout,object recognition,optical flow estimation,Optical imaging,pattern recognition algorithms,per-pixel brightness changes,spatiotemporal phenomena,Spatiotemporal phenomena,Standards,Task analysis,task dedicated network,vision sensors}
}

@book{gerstner2002spiking,
  title = {Spiking Neuron Models: {{Single}} Neurons, Populations, Plasticity},
  author = {Gerstner, Wulfram and Kistler, Werner M},
  date = {2002},
  publisher = {{Cambridge university press}}
}

@article{gibson18_neural_ordin_differ_equat,
  title = {Neural Networks as Ordinary Differential Equations},
  author = {Gibson, Kevin},
  date = {2018},
  url = {https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/},
  note = {Online; accessed 28 February 2019}
}

@inproceedings{googlecartographer,
  title = {Real-Time Loop Closure in {{2D LIDAR SLAM}}},
  booktitle = {2016 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Hess, Wolfgang and Kohler, Damon and Rapp, Holger and Andor, Daniel},
  date = {2016},
  pages = {1271--1278}
}

@article{gu16_q_prop,
  title = {Q-Prop: {{Sample}}-Efficient Policy Gradient with an off-Policy Critic},
  author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
  date = {2016},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1611.02247v3},
  abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
  archivePrefix = {arXiv},
  eprint = {1611.02247},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{guetig14_to_spike_or_when_to_spike,
  title = {To Spike, or When to Spike?},
  author = {Gütig, Robert},
  date = {2014},
  journaltitle = {Current Opinion in Neurobiology},
  volume = {25},
  pages = {134--139},
  doi = {10.1016/j.conb.2014.01.004},
  url = {https://doi.org/10.1016/j.conb.2014.01.004},
  date_added = {Fri Nov 1 16:23:04 2019},
  issue = {nil}
}

@article{guo17_deepf,
  title = {Deepfm: A Factorization-Machine Based Neural Network for Ctr Prediction},
  author = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1703.04247v1},
  abstract = {Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.},
  archivePrefix = {arXiv},
  eprint = {1703.04247},
  eprinttype = {arxiv},
  primaryClass = {cs.IR}
}

@article{haan19_causal_confus_imitat_learn,
  title = {Causal Confusion in Imitation Learning},
  author = {de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1905.11979v2},
  abstract = {Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive "causal misidentification" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions—either environment interaction or expert queries—to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.},
  archivePrefix = {arXiv},
  eprint = {1905.11979},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{haber17_stabl_archit_deep_neural_networ,
  title = {Stable Architectures for Deep Neural Networks},
  author = {Haber, Eldad and Ruthotto, Lars},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1705.03341v3},
  abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks. The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability and well-posedness of deep learning and use this new understanding to develop new network architectures. We relate the exploding and vanishing gradient phenomenon to the stability of the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments show their competitiveness with state-of-the-art networks.},
  archivePrefix = {arXiv},
  eprint = {1705.03341},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{handmade_how_to_write_better,
  title = {How to Write Better (Game) Libraries | Handmade.Network {{Wiki}}},
  author = {{Handmade}},
  date = {2019},
  url = {https://handmade.network/wiki/7138-howₜo<sub>w</sub>rite<sub>b</sub>etter<sub>g</sub>ameₗibraries},
  note = {Online; accessed 12 December 2019}
}

@article{haug18_teach_inver_reinf_learn_via_featur_demon,
  title = {Teaching Inverse Reinforcement Learners via Features and Demonstrations},
  author = {Haug, Luis and Tschiatschek, Sebastian and Singla, Adish},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1810.08926v4},
  abstract = {Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.},
  archivePrefix = {arXiv},
  eprint = {1810.08926},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{he15_deep_resid_learn_image_recog,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1512.03385v1},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57 \% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 \% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{heeger2000poisson,
  title = {Poisson Model of Spike Generation},
  author = {Heeger, David},
  date = {2000},
  journaltitle = {Handout, University of Standford},
  volume = {5},
  pages = {1--13}
}

@online{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020-03-23},
  url = {http://arxiv.org/abs/1911.05722},
  urldate = {2020-07-08},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archivePrefix = {arXiv},
  eprint = {1911.05722},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/2DPSAS9V/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf;/home/jethro/Zotero/storage/VP3WILQH/1911.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: CVPR 2020 camera-ready. Code: https://github.com/facebookresearch/moco},
  primaryClass = {cs}
}

@online{hjelmLearningDeepRepresentations2019,
  title = {Learning Deep Representations by Mutual Information Estimation and Maximization},
  author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  date = {2019-02-22},
  url = {http://arxiv.org/abs/1808.06670},
  urldate = {2020-07-08},
  abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
  archivePrefix = {arXiv},
  eprint = {1808.06670},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/AX9WY9H5/Hjelm et al. - 2019 - Learning deep representations by mutual informatio.pdf;/home/jethro/Zotero/storage/43T4KEQQ/1808.html},
  keywords = {Computer Science - Machine Learning,Contrastive Learning,Statistics - Machine Learning},
  note = {Comment: Accepted as an oral presentation at the International Conference for Learning Representations (ICLR), 2019},
  primaryClass = {cs, stat}
}

@article{home_cookiec_data_scien,
  title = {Home - Cookiecutter Data Science},
  author = {{DrivenData}},
  date = {2019},
  url = {https://drivendata.github.io/cookiecutter-data-science/},
  note = {Online; accessed 06 January 2019}
}

@article{home_keras_docum,
  title = {Home Keras Documentation},
  author = {{Keras}},
  date = {2019},
  url = {https://keras.io/},
  note = {Online; accessed 08 January 2019}
}

@article{horgan18_distr_prior_exper_replay,
  title = {Distributed Prioritized Experience Replay},
  author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1803.00933v1},
  abstract = {We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.},
  archivePrefix = {arXiv},
  eprint = {1803.00933},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{houghton18_calcul_mutual_infor_between_two_spike_train,
  title = {Calculating the Mutual Information between Two Spike Trains},
  author = {Houghton, Conor},
  date = {2018},
  doi = {10.1101/423608},
  url = {https://doi.org/10.1101/423608},
  date_added = {Thu Jan 23 11:52:27 2020}
}

@article{huang18_gpipe,
  title = {Gpipe: {{Efficient}} Training of Giant Neural Networks Using Pipeline Parallelism},
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1811.06965v5},
  abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4 \% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
  archivePrefix = {arXiv},
  eprint = {1811.06965},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@incollection{huhGradientDescentSpiking2018,
  title = {Gradient Descent for Spiking Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Huh, Dongsung and Sejnowski, Terrence J},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {1433--1443},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7417-gradient-descent-for-spiking-neural-networks.pdf},
  file = {/home/jethro/Zotero/storage/NPSS2Y37/Huh and Sejnowski - 2018 - Gradient descent for spiking neural networks.pdf}
}

@article{ivanov19_moder_deep_reinf_learn_algor,
  title = {Modern Deep Reinforcement Learning Algorithms},
  author = {Ivanov, Sergey and D'yakonov, Alexander},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1906.10025v2},
  abstract = {Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many artificial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a field of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justification, practical limitations and observed empirical properties.},
  archivePrefix = {arXiv},
  eprint = {1906.10025},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@online{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2019-02-25},
  url = {http://arxiv.org/abs/1803.05407},
  urldate = {2020-08-25},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archivePrefix = {arXiv},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Appears at the Conference on Uncertainty in Artificial Intelligence (UAI), 2018},
  primaryClass = {cs, stat}
}

@book{james2013introduction,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2013},
  volume = {112},
  publisher = {{Springer}}
}

@article{jang18_introd_to_spikin_neural_networ,
  title = {An Introduction to Spiking Neural Networks: {{Probabilistic}} Models, Learning Rules, and Applications},
  author = {Jang, Hyeryung and Simeone, Osvaldo and Gardner, Brian and Grüning, André},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1812.03929v4},
  abstract = {Spiking Neural Networks (SNNs) are distributed trainable systems whose computing elements, or neurons, are characterized by internal analog dynamics and by digital and sparse synaptic communications. The sparsity of the synaptic spiking inputs and the corresponding event-driven nature of neural processing can be leveraged by hardware implementations that have demonstrated significant energy reductions as compared to conventional Artificial Neural Networks (ANNs). Most existing training algorithms for SNNs have been designed either for biological plausibility or through conversion from pre-trained ANNs via rate encoding. This paper aims at providing an introduction to SNNs by focusing on a probabilistic signal processing methodology that enables the direct derivation of learning rules leveraging the unique time encoding capabilities of SNNs. To this end, the paper adopts discrete-time probabilistic models for networked spiking neurons, and it derives supervised and unsupervised learning rules from first principles by using variational inference. Examples and open research problems are also provided.},
  archivePrefix = {arXiv},
  eprint = {1812.03929v4},
  eprinttype = {arxiv},
  primaryClass = {eess.SP}
}

@online{jaskowskiImprovedGQCNNDeep2018,
  title = {Improved {{GQ}}-{{CNN}}: {{Deep Learning Model}} for {{Planning Robust Grasps}}},
  shorttitle = {Improved {{GQ}}-{{CNN}}},
  author = {Jaśkowski, Maciej and Świątkowski, Jakub and Zając, Michał and Klimek, Maciej and Potiuk, Jarek and Rybicki, Piotr and Polatowski, Piotr and Walczyk, Przemysław and Nowicki, Kacper and Cygan, Marek},
  date = {2018-02-16},
  url = {http://arxiv.org/abs/1802.05992},
  urldate = {2020-07-22},
  abstract = {Recent developments in the field of robot grasping have shown great improvements in the grasp success rates when dealing with unknown objects. In this work we improve on one of the most promising approaches, the Grasp Quality Convolutional Neural Network (GQ-CNN) trained on the DexNet 2.0 dataset. We propose a new architecture for the GQ-CNN and describe practical improvements that increase the model validation accuracy from 92.2\% to 95.8\% and from 85.9\% to 88.0\% on respectively image-wise and object-wise training and validation splits.},
  archivePrefix = {arXiv},
  eprint = {1802.05992},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/K5HDHIPE/Jaśkowski et al. - 2018 - Improved GQ-CNN Deep Learning Model for Planning .pdf;/home/jethro/Zotero/storage/A9NX64TT/1802.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  note = {Comment: 6 pages, 3 figures},
  primaryClass = {cs, stat}
}

@incollection{jin_q_learning_provably_efficient,
  title = {Is Q-Learning Provably Efficient?},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {4863--4873},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7735-is-q-learning-provably-efficient.pdf}
}

@online{jingSelfsupervisedVisualFeature2019,
  title = {Self-Supervised {{Visual Feature Learning}} with {{Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Self-Supervised {{Visual Feature Learning}} with {{Deep Neural Networks}}},
  author = {Jing, Longlong and Tian, Yingli},
  date = {2019-02-16},
  url = {http://arxiv.org/abs/1902.06162},
  urldate = {2020-07-08},
  abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
  archivePrefix = {arXiv},
  eprint = {1902.06162},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/QAEMMAE7/Jing and Tian - 2019 - Self-supervised Visual Feature Learning with Deep .pdf;/home/jethro/Zotero/storage/LRJG7TI7/1902.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{johndrow15_optim_approx_markov_chain_bayes_infer,
  title = {Optimal Approximating Markov Chains for Bayesian Inference},
  author = {Johndrow, James E. and Mattingly, Jonathan C. and Mukherjee, Sayan and Dunson, David},
  date = {2015},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1508.03387v3},
  abstract = {The Markov Chain Monte Carlo method is the dominant paradigm for posterior computation in Bayesian analysis. It is common to control computation time by making approximations to the Markov transition kernel. Comparatively little attention has been paid to computational optimality in these approximating Markov Chains, or when such approximations are justified relative to obtaining shorter paths from the exact kernel. We give simple, sharp bounds for uniform approximations of uniformly mixing Markov chains. We then suggest a notion of optimality that incorporates computation time and approximation error, and use our bounds to make generalizations about properties of good approximations in the uniformly mixing setting. The relevance of these properties is demonstrated in applications to a minibatching-based approximate MCMC algorithm for large n logistic regression and low-rank approximations for Gaussian processes.},
  archivePrefix = {arXiv},
  eprint = {1508.03387},
  eprinttype = {arxiv},
  primaryClass = {stat.CO}
}

@article{johndrow17_bayes_shrin_at_gwas_scale,
  title = {Bayes Shrinkage at Gwas Scale: {{Convergence}} and Approximation Theory of a Scalable Mcmc Algorithm for the Horseshoe Prior},
  author = {Johndrow, James E. and Orenstein, Paulo and Bhattacharya, Anirban},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1705.00841v3},
  abstract = {The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove that the exact algorithm is geometrically ergodic, and give guarantees for the accuracy of the approximate algorithm using perturbation theory. Versions of the approximation algorithm that gradually decrease the approximation error as the chain extends are shown to be exact. The scalability of the algorithm is illustrated in simulations with problem size as large as N=5,000 observations and p=50,000 predictors, and an application to a genome-wide association study with N=2,267 and p=98,385. The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model.},
  archivePrefix = {arXiv},
  eprint = {1705.00841},
  eprinttype = {arxiv},
  primaryClass = {stat.CO}
}

@article{kaelbling1996reinforcement,
  title = {Reinforcement Learning: {{A}} Survey},
  author = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  date = {1996},
  journaltitle = {Journal of artificial intelligence research},
  volume = {4},
  pages = {237--285}
}

@article{kamalaruban19_inter_teach_algor_inver_reinf_learn,
  title = {Interactive Teaching Algorithms for Inverse Reinforcement Learning},
  author = {Kamalaruban, Parameswaran and Devidze, Rati and Cevher, Volkan and Singla, Adish},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1905.11867v3},
  abstract = {We study the problem of inverse reinforcement learning (IRL) with the added twist that the learner is assisted by a helpful teacher. More formally, we tackle the following algorithmic question: How could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process? We present an interactive teaching framework where a teacher adaptively chooses the next demonstration based on learner's current policy. In particular, we design teaching algorithms for two concrete settings: an omniscient setting where a teacher has full knowledge about the learner's dynamics and a blackbox setting where the teacher has minimal knowledge. Then, we study a sequential variant of the popular MCE-IRL learner and prove convergence guarantees of our teaching algorithm in the omniscient setting. Extensive experiments with a car driving simulator environment show that the learning progress can be speeded up drastically as compared to an uninformative teacher.},
  archivePrefix = {arXiv},
  eprint = {1905.11867},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@inproceedings{klaus_greff-proc-scipy-2017,
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  booktitle = {Proceedings of the 16th {{Python}} in {{Science Conference}}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and {Jürgen Schmidhuber}},
  editor = {Huff, Katy and Lippa, David and Niederhut, Dillon and {M Pacer}},
  date = {2017},
  pages = {49--56},
  doi = {10.25080/shinma-7f4c6e7-008}
}

@book{koller2009probabilistic,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  author = {Koller, Daphne and Friedman, Nir and Bach, Francis},
  date = {2009},
  publisher = {{MIT press}}
}

@article{lagorceHOTSHierarchyEventBased2017,
  title = {{{HOTS}}: {{A Hierarchy}} of {{Event}}-{{Based Time}}-{{Surfaces}} for {{Pattern Recognition}}},
  shorttitle = {{{HOTS}}},
  author = {Lagorce, Xavier and Orchard, Garrick and Galluppi, Francesco and Shi, Bertram E. and Benosman, Ryad B.},
  date = {2017-07},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  pages = {1346--1359},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2574707},
  abstract = {This paper describes novel event-based spatio-temporal features called time-surfaces and how they can be used to create a hierarchical event-based pattern recognition architecture. Unlike existing hierarchical architectures for pattern recognition, the presented model relies on a time oriented approach to extract spatio-temporal features from the asynchronously acquired dynamics of a visual scene. These dynamics are acquired using biologically inspired frameless asynchronous event-driven vision sensors. Similarly to cortical structures, subsequent layers in our hierarchy extract increasingly abstract features using increasingly large spatio-temporal windows. The central concept is to use the rich temporal information provided by events to create contexts in the form of time-surfaces which represent the recent temporal activity within a local spatial neighborhood. We demonstrate that this concept can robustly be used at all stages of an event-based hierarchical model. First layer feature units operate on groups of pixels, while subsequent layer feature units operate on the output of lower level feature units. We report results on a previously published 36 class character recognition task and a four class canonical dynamic card pip task, achieving near 100 percent accuracy on each. We introduce a new seven class moving face recognition task, achieving 79 percent accuracy.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/jethro/Zotero/storage/GCNVZRA5/Lagorce et al. - 2017 - HOTS A Hierarchy of Event-Based Time-Surfaces for.pdf;/home/jethro/Zotero/storage/UBRW4J7A/7508476.html},
  keywords = {asynchronously acquired dynamics,biologically inspired vision sensors,Biosensors,Cameras,canonical dynamic card pip task,Character recognition,computer vision,cortical structures,event-based spatio-temporal features,event-based vision,face recognition,feature extraction,Feature extraction,frameless asynchronous event-driven vision sensors,hierarchical event-based pattern recognition architecture,hierarchy of event-based time-surfaces,HOTS,image sensors,Neuromorphic sensing,Object recognition,seven class moving face recognition task,time-surfaces,visual scene,Visualization},
  number = {7}
}

@book{lakdawalla18_curios,
  title = {The Design and Engineering of {{Curiosity}} : How the {{Mars Rover}} Performs Its Job},
  author = {Lakdawalla, Emily},
  date = {2018},
  publisher = {{Springer}},
  location = {{Cham, Switzerland}},
  isbn = {978-3-319-68146-7}
}

@online{leeSpikeFlowNetEventbasedOptical2020,
  title = {Spike-{{FlowNet}}: {{Event}}-Based {{Optical Flow Estimation}} with {{Energy}}-{{Efficient Hybrid Neural Networks}}},
  shorttitle = {Spike-{{FlowNet}}},
  author = {Lee, Chankyu and Kosta, Adarsh and Zhu, Alex Zihao and Chaney, Kenneth and Daniilidis, Kostas and Roy, Kaushik},
  date = {2020-03-14},
  url = {http://arxiv.org/abs/2003.06696},
  urldate = {2020-07-08},
  abstract = {Event-based cameras display great potential for a variety of conditions such as high-speed motion detection and enabling navigation in low-light environments where conventional frame-based cameras suffer critically. This is attributed to their high temporal resolution, high dynamic range, and low-power consumption. However, conventional computer vision methods as well as deep Analog Neural Networks (ANNs) are not suited to work well with the asynchronous and discrete nature of event camera outputs. Spiking Neural Networks (SNNs) serve as ideal paradigms to handle event camera outputs, but deep SNNs suffer in terms of performance due to spike vanishing phenomenon. To overcome these issues, we present Spike-FlowNet, a deep hybrid neural network architecture integrating SNNs and ANNs for efficiently estimating optical flow from sparse event camera outputs without sacrificing the performance. The network is end-to-end trained with self-supervised learning on Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Spike-FlowNet outperforms its corresponding ANN-based method in terms of the optical flow prediction capability while providing significant computational efficiency.},
  archivePrefix = {arXiv},
  eprint = {2003.06696},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/ZNYBIVET/Lee et al. - 2020 - Spike-FlowNet Event-based Optical Flow Estimation.pdf;/home/jethro/Zotero/storage/CMAA4X56/2003.html},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{levine18_reinf_learn_contr_as_probab_infer,
  title = {Reinforcement Learning and Control as Probabilistic Inference: {{Tutorial}} and Review},
  author = {Levine, Sergey},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1805.00909v3},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archivePrefix = {arXiv},
  eprint = {1805.00909},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{levineLearningHandeyeCoordination2017,
  title = {Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection:},
  shorttitle = {Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection},
  author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
  date = {2017-06-12},
  journaltitle = {The International Journal of Robotics Research},
  publisher = {{SAGE PublicationsSage UK: London, England}},
  doi = {10.1177/0278364917710318},
  url = {https://journals.sagepub.com/doi/10.1177/0278364917710318},
  urldate = {2020-07-13},
  abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we train...},
  file = {/home/jethro/Zotero/storage/EQS6PB8H/Levine et al. - 2017 - Learning hand-eye coordination for robotic graspin.pdf;/home/jethro/Zotero/storage/U498U8QF/0278364917710318.html},
  langid = {english}
}

@article{li16_simpl_scalab_accur_poster_inter_estim,
  title = {Simple, Scalable and Accurate Posterior Interval Estimation},
  author = {Li, Cheng and Srivastava, Sanvesh and Dunson, David B.},
  date = {2016},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1605.04029v2},
  abstract = {There is a lack of simple and scalable algorithms for uncertainty quantification. Bayesian methods quantify uncertainty through posterior and predictive distributions, but it is difficult to rapidly estimate summaries of these distributions, such as quantiles and intervals. Variational Bayes approximations are widely used, but may badly underestimate posterior covariance. Typically, the focus of Bayesian inference is on point and interval estimates for one-dimensional functionals of interest. In small scale problems, Markov chain Monte Carlo algorithms remain the gold standard, but such algorithms face major problems in scaling up to big data. Various modifications have been proposed based on parallelization and approximations based on subsamples, but such approaches are either highly complex or lack theoretical support and/or good performance outside of narrow settings. We propose a very simple and general posterior interval estimation algorithm, which is based on running Markov chain Monte Carlo in parallel for subsets of the data and averaging quantiles estimated from each subset. We provide strong theoretical guarantees and illustrate performance in several applications.},
  archivePrefix = {arXiv},
  eprint = {1605.04029},
  eprinttype = {arxiv},
  primaryClass = {stat.CO}
}

@article{li18_deep_reinf_learn,
  title = {Deep Reinforcement Learning},
  author = {Li, Yuxi},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1810.06339v1},
  abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  archivePrefix = {arXiv},
  eprint = {1810.06339},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{lian18_xdeep,
  title = {Xdeepfm: {{Combining}} Explicit and Implicit Feature Interactions for Recommender Systems},
  author = {Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1803.05170v3},
  abstract = {Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.},
  archivePrefix = {arXiv},
  eprint = {1803.05170},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{liEventbasedRoboticGrasping2020,
  title = {Event-Based {{Robotic Grasping Detection}} with {{Neuromorphic Vision Sensor}} and {{Event}}-{{Stream Dataset}}},
  author = {Li, Bin and Cao, Hu and Qu, Zhongnan and Hu, Yingbai and Wang, Zhanguo and Liang, Zichen},
  date = {2020},
  journaltitle = {ArXiv},
  abstract = {Robotic grasping plays an important role in the field of robotics. The current state-of-the-art robotic grasping detection systems are usually built on the conventional vision, such as RGB-D camera. Compared to traditional frame-based computer vision, neuromorphic vision is a small and young community of research. Currently, there are limited event-based datasets due to the troublesome annotation of the asynchronous event stream. Annotating large scale vision dataset often takes lots of computation resources, especially the troublesome data for video-level annotation. In this work, we consider the problem of detecting robotic grasps in a moving camera view of a scene containing objects. To obtain more agile robotic perception, a neuromorphic vision sensor (DAVIS) attaching to the robot gripper is introduced to explore the potential usage in grasping detection. We construct a robotic grasping dataset named Event-Stream Dataset with 91 objects. A spatio-temporal mixed particle filter (SMP Filter) is proposed to track the led-based grasp rectangles which enables video-level annotation of a single grasp rectangle per object. As leds blink at high frequency, the Event-Stream dataset is annotated in a high frequency of 1 kHz. Based on the Event-Stream dataset, we develop a deep neural network for grasping detection which consider the angle learning problem as classification instead of regression. The method performs high detection accuracy on our Event-Stream dataset with 93\% precision at object-wise level. This work provides a large-scale and well-annotated dataset, and promotes the neuromorphic vision applications in agile robot.},
  file = {/home/jethro/Zotero/storage/U4ZCQM2R/Li et al. - 2020 - Event-based Robotic Grasping Detection with Neurom.pdf}
}

@article{lilian_domain_random_sim2r_trans,
  title = {Domain Randomization for {{Sim2Real}} Transfer},
  author = {Weng, Lilian},
  date = {2019},
  url = {https://lilianweng.github.io/lil-log/2019/05/05/domain-randomization.html},
  note = {Online; accessed 28 June 2019}
}

@article{liuEventDrivenSensingEfficient2019,
  title = {Event-{{Driven Sensing}} for {{Efficient Perception}}: {{Vision}} and {{Audition Algorithms}}},
  shorttitle = {Event-{{Driven Sensing}} for {{Efficient Perception}}},
  author = {Liu, Shih-Chii and Rueckauer, Bodo and Ceolini, Enea and Huber, Adrian and Delbruck, Tobi},
  date = {2019-11},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {36},
  pages = {29--37},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2019.2928127},
  url = {https://ieeexplore.ieee.org/document/8887562/},
  urldate = {2020-08-04},
  file = {/home/jethro/Zotero/storage/3EKENWUB/Liu et al. - 2019 - Event-Driven Sensing for Efficient Perception Vis.pdf},
  langid = {english},
  number = {6}
}

@article{liuEventDrivenSensingEfficient2019a,
  title = {Event-{{Driven Sensing}} for {{Efficient Perception}}: {{Vision}} and {{Audition Algorithms}}},
  shorttitle = {Event-{{Driven Sensing}} for {{Efficient Perception}}},
  author = {Liu, Shih-Chii and Rueckauer, Bodo and Ceolini, Enea and Huber, Adrian and Delbruck, Tobi},
  date = {2019-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {36},
  pages = {29--37},
  issn = {1558-0792},
  doi = {10.1109/MSP.2019.2928127},
  abstract = {Event sensors implement circuits that capture partial functionality of biological sensors, such as the retina and cochlea. As with their biological counterparts, event sensors are drivers of their own output. That is, they produce dynamically sampled binary events to dynamically changing stimuli. Algorithms and networks that process this form of output representation are still in their infancy, but they show strong promise. This article illustrates the unique form of the data produced by the sensors and demonstrates how the properties of these sensor outputs make them useful for power-efficient, low-latency systems working in real time.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  file = {/home/jethro/Zotero/storage/2JV5XZWS/8887562.html},
  keywords = {audition algorithm,binary events,biological counterparts,biological sensors,Biosensors,capture partial functionality,Chemical sensors,cochlea,efficient perception,event sensors,event-driven sensing,Heuristic algorithms,neural nets,output representation,physiological models,retina,Retina,sensor outputs,Sensor systems,Signal processing algorithms,vision algorithm,visual perception,Voltage control},
  number = {6}
}

@article{lu17_beyon_finit_layer_neural_networ,
  title = {Beyond Finite Layer Neural Networks: {{Bridging}} Deep Architectures and Numerical Differential Equations},
  author = {Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1710.10121v2},
  abstract = {In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress ({$>$}50 \%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.},
  archivePrefix = {arXiv},
  eprint = {1710.10121},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{MAASS19971659,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  author = {Maass, Wolfgang},
  date = {1997},
  journaltitle = {Neural Networks},
  volume = {10},
  pages = {1659--1671},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(97)00011-7},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608097000117},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
  keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
  number = {9}
}

@article{maciver_hard_things,
  title = {How to Do Hard Things},
  author = {MacIver, David R.},
  date = {2019},
  url = {https://www.drmaciver.com/2019/05/how-to-do-hard-things/},
  note = {Online; accessed 20 May 2019}
}

@online{mahlerDexNetDeepLearning2017,
  title = {Dex-{{Net}} 2.0: {{Deep Learning}} to {{Plan Robust Grasps}} with {{Synthetic Point Clouds}} and {{Analytic Grasp Metrics}}},
  shorttitle = {Dex-{{Net}} 2.0},
  author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
  date = {2017-08-08},
  url = {http://arxiv.org/abs/1703.09312},
  urldate = {2020-07-28},
  abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93\% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
  archivePrefix = {arXiv},
  eprint = {1703.09312},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/59MPMXVX/Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps w.pdf;/home/jethro/Zotero/storage/APVRTR5W/1703.html},
  keywords = {Computer Science - Robotics},
  note = {Comment: To appear at Robotics: Science and Systems 2017},
  primaryClass = {cs}
}

@online{mahlerEfficientPolicyLearning2018,
  title = {Efficient {{Policy Learning}} for {{Robust Robot Grasping}}},
  author = {Mahler, Jeffrey},
  date = {2018},
  journaltitle = {undefined},
  url = {/paper/Efficient-Policy-Learning-for-Robust-Robot-Grasping-Mahler/b32f3a46f6d2058699e44bf38d9aa0cc66025f24},
  urldate = {2020-07-22},
  abstract = {Author(s): Mahler, Jeffrey Brian | Advisor(s): Goldberg, Ken | Abstract: While humans can grasp and manipulate novel objects with ease, rapid and reliable robot grasping of a wide variety of objects is highly challenging due to sensor noise, partial observability, imprecise control, and hardware limitations.Analytic approaches to robot grasping use models from physics to predict grasp success but require precise knowledge of the robot and objects in the environment, making them well-suited for controlled industrial applications but difficult to scale to many objects.On the other hand, deep neural networks trained on large datasets of grasps labeled with empirical successes and failures can rapidly plan grasps across a diverse set of objects, but data collection is tedious, robot-specific, and prone to mislabeling.To improve the efficiency of learning deep grasping policies, we propose a hybrid method to automate dataset collection by generating millions of synthetic 3D point clouds, robot grasps, and success metrics using analytic models of contact, collision geometry, and image formation.We present the Dexterity-Network (Dex-Net), a framework for generating training datasets by analyzing mechanical models of contact forces and torques under stochastic perturbations across thousands of 3D object CAD models.We describe dataset generation models for training policies to lift and transport novel objects from a tabletop or cluttered bin using a 3D depth sensor and a parallel-jaw (two-finger) or suction cup gripper.To study the effects of learning from massive amounts of training data, we generate datasets containing millions of training examples using distributed Cloud computing, simulations, and parallel GPU processing.We use these datasets to train robust grasping policies based on Grasp Quality Convolutional Neural Networks (GQ-CNNs) that take as input a depth image and a candidate grasp with up to five degrees of freedom and predict the probability of grasp success on an object in the image.To transfer from simulation to reality, we develop novel analytic grasp success metrics based on resisting disturbing forces and torques under stochastic perturbations and bounding an object\&\#39;s mobility under an energy field such as gravity. In addition, we study techniques in algorithmic supervision to guide dataset collection using full knowledge of the object geometry and pose in simulation.We explore extensions to learning policies that sequentially pick novel objects from dense clutter in a bin and that can rapidly decide which gripper hardware is best in a particular scenario.To substantiate the method, we describe thousands of experimental trials on a physical robot which suggest that deep learning on synthetic Dex-Net datasets can be used to rapidly and reliably plan grasps across a diverse set of novel objects for a variety of depth sensors, robot grippers, and robot arms.Results suggest that policies trained on Dex-Net datasets can achieve up to 95\% success in picking novel objects from densely cluttered bins at a rate of over 310 mean picks per hour with no additional training or tuning on the physical system.},
  file = {/home/jethro/Zotero/storage/J7I7WA9B/b32f3a46f6d2058699e44bf38d9aa0cc66025f24.html},
  langid = {english}
}

@inproceedings{manderscheidSpeedInvariantTime2019,
  title = {Speed {{Invariant Time Surface}} for {{Learning}} to {{Detect Corner Points With Event}}-{{Based Cameras}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Manderscheid, Jacques and Sironi, Amos and Bourdis, Nicolas and Migliore, Davide and Lepetit, Vincent},
  date = {2019-06},
  pages = {10237--10246},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01049},
  url = {https://ieeexplore.ieee.org/document/8954376/},
  urldate = {2020-06-29},
  abstract = {We propose a learning approach to corner detection for event-based cameras that is stable even under fast and abrupt motions. Event-based cameras offer high temporal resolution, power efficiency, and high dynamic range. However, the properties of event-based data are very different compared to standard intensity images, and simple extensions of corner detection methods designed for these images do not perform well on event-based data. We first introduce an efficient way to compute a time surface that is invariant to the speed of the objects. We then show that we can train a Random Forest to recognize events generated by a moving corner from our time surface. Random Forests are also extremely efficient, and therefore a good choice to deal with the high capture frequency of event-based cameras—our implementation processes up to 1.6Mev/s on a single CPU. Thanks to our time surface formulation and this learning approach, our method is significantly more robust to abrupt changes of direction of the corners compared to previous ones. Our method also naturally assigns a confidence score for the corners, which can be useful for postprocessing. Moreover, we introduce a highresolution dataset suitable for quantitative evaluation and comparison of corner detection methods for event-based cameras. We call our approach SILC, for Speed Invariant Learned Corners, and compare it to the state-of-the-art with extensive experiments, showing better performance.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/home/jethro/Zotero/storage/CBLF3SCR/Manderscheid et al. - 2019 - Speed Invariant Time Surface for Learning to Detec.pdf;/home/jethro/Zotero/storage/IZX34LCF/manderscheid2019.pdf},
  isbn = {978-1-72813-293-8},
  langid = {english}
}

@article{Merolla668,
  title = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  date = {2014},
  journaltitle = {Science},
  volume = {345},
  pages = {668--673},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1254642},
  url = {https://science.sciencemag.org/content/345/6197/668},
  abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brains structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  eprint = {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  number = {6197}
}

@article{mishraSaccadeBasedFramework2017,
  title = {A {{Saccade Based Framework}} for {{Real}}-{{Time Motion Segmentation Using Event Based Vision Sensors}}},
  author = {Mishra, Abhishek and Ghosh, Rohan and Principe, Jose C. and Thakor, Nitish V. and Kukreja, Sunil L.},
  date = {2017-03-03},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {11},
  issn = {1662-453X},
  doi = {10.3389/fnins.2017.00083},
  url = {http://journal.frontiersin.org/article/10.3389/fnins.2017.00083/full},
  urldate = {2020-06-29},
  abstract = {Motion segmentation is a critical pre-processing step for autonomous robotic systems to facilitate tracking of moving objects in cluttered environments. Event based sensors are low power analog devices that represent a scene by means of asynchronous information updates of only the dynamic details at high temporal resolution and, hence, require significantly less calculations. However, motion segmentation using spatiotemporal data is a challenging task due to data asynchrony. Prior approaches for object tracking using neuromorphic sensors perform well while the sensor is static or a known model of the object to be followed is available. To address these limitations, in this paper we develop a technique for generalized motion segmentation based on spatial statistics across time frames. First, we create micromotion on the platform to facilitate the separation of static and dynamic elements of a scene, inspired by human saccadic eye movements. Second, we introduce the concept of spike-groups as a methodology to partition spatio-temporal event groups, which facilitates computation of scene statistics and characterize objects in it. Experimental results show that our algorithm is able to classify dynamic objects with a moving camera with maximum accuracy of 92\%.},
  file = {/home/jethro/Zotero/storage/CE735E8W/Mishra et al. - 2017 - A Saccade Based Framework for Real-Time Motion Seg.pdf;/home/jethro/Zotero/storage/F2YHAHKA/mishra2017.pdf},
  langid = {english}
}

@article{Mnih_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and {al.}, et},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  pages = {529--533},
  publisher = {{Springer Nature}},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {http://dx.doi.org/10.1038/nature14236},
  number = {7540}
}

@article{mnih16_async_method_deep_reinf_learn,
  title = {Asynchronous Methods for Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1602.01783v2},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archivePrefix = {arXiv},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@online{mnih2013playing,
  title = {Playing Atari with Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013},
  archivePrefix = {arXiv},
  eprint = {1312.5602},
  eprinttype = {arxiv}
}

@online{morrisonClosingLoopRobotic2018,
  title = {Closing the {{Loop}} for {{Robotic Grasping}}: {{A Real}}-Time, {{Generative Grasp Synthesis Approach}}},
  shorttitle = {Closing the {{Loop}} for {{Robotic Grasping}}},
  author = {Morrison, Douglas and Corke, Peter and Leitner, Jürgen},
  date = {2018-05-15},
  url = {http://arxiv.org/abs/1804.05172},
  urldate = {2020-07-22},
  abstract = {This paper presents a real-time, object-independent grasp synthesis method which can be used for closed-loop grasping. Our proposed Generative Grasping Convolutional Neural Network (GG-CNN) predicts the quality and pose of grasps at every pixel. This one-to-one mapping from a depth image overcomes limitations of current deep-learning grasping techniques by avoiding discrete sampling of grasp candidates and long computation times. Additionally, our GG-CNN is orders of magnitude smaller while detecting stable grasps with equivalent performance to current state-of-the-art techniques. The light-weight and single-pass generative nature of our GG-CNN allows for closed-loop control at up to 50Hz, enabling accurate grasping in non-static environments where objects move and in the presence of robot control inaccuracies. In our real-world tests, we achieve an 83\% grasp success rate on a set of previously unseen objects with adversarial geometry and 88\% on a set of household objects that are moved during the grasp attempt. We also achieve 81\% accuracy when grasping in dynamic clutter.},
  archivePrefix = {arXiv},
  eprint = {1804.05172},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/CBZ9V2LL/Morrison et al. - 2018 - Closing the Loop for Robotic Grasping A Real-time.pdf;/home/jethro/Zotero/storage/JMMJ6IYI/1804.html},
  keywords = {Computer Science - Robotics},
  note = {Comment: Robotics: Science and Systems (RSS), 2018. Code: http://github.com/dougsm/ggcnn Video: http://www.youtube.com/watch?v=7nOoxuGEcxA},
  primaryClass = {cs}
}

@online{morrisonClosingLoopRobotic2018a,
  title = {Closing the {{Loop}} for {{Robotic Grasping}}: {{A Real}}-Time, {{Generative Grasp Synthesis Approach}}},
  shorttitle = {Closing the {{Loop}} for {{Robotic Grasping}}},
  author = {Morrison, Douglas and Corke, Peter and Leitner, Jürgen},
  date = {2018-05-15},
  url = {http://arxiv.org/abs/1804.05172},
  urldate = {2020-07-20},
  abstract = {This paper presents a real-time, object-independent grasp synthesis method which can be used for closed-loop grasping. Our proposed Generative Grasping Convolutional Neural Network (GG-CNN) predicts the quality and pose of grasps at every pixel. This one-to-one mapping from a depth image overcomes limitations of current deep-learning grasping techniques by avoiding discrete sampling of grasp candidates and long computation times. Additionally, our GG-CNN is orders of magnitude smaller while detecting stable grasps with equivalent performance to current state-of-the-art techniques. The light-weight and single-pass generative nature of our GG-CNN allows for closed-loop control at up to 50Hz, enabling accurate grasping in non-static environments where objects move and in the presence of robot control inaccuracies. In our real-world tests, we achieve an 83\% grasp success rate on a set of previously unseen objects with adversarial geometry and 88\% on a set of household objects that are moved during the grasp attempt. We also achieve 81\% accuracy when grasping in dynamic clutter.},
  archivePrefix = {arXiv},
  eprint = {1804.05172},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/9VD3F4AL/Morrison et al. - 2018 - Closing the Loop for Robotic Grasping A Real-time.pdf;/home/jethro/Zotero/storage/V8TICWQP/1804.html},
  keywords = {Computer Science - Robotics},
  note = {Comment: Robotics: Science and Systems (RSS), 2018. Code: http://github.com/dougsm/ggcnn Video: http://www.youtube.com/watch?v=7nOoxuGEcxA},
  primaryClass = {cs}
}

@article{murphy2014machine,
  title = {Machine Learning: A Probabilistic Perspective. 2012},
  author = {Murphy, Kevin P},
  date = {2014},
  journaltitle = {Cité en},
  pages = {117}
}

@article{nair15_massiv_paral_method_deep_reinf_learn,
  title = {Massively Parallel Methods for Deep Reinforcement Learning},
  author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and Maria, Alessandro De and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
  date = {2015},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1507.04296v2},
  abstract = {We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.},
  archivePrefix = {arXiv},
  eprint = {1507.04296},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{nakano11_spikin_neural_networ_model_free,
  title = {Spiking Neural Network Model of Free-Energy-Based Reinforcement Learning},
  author = {Nakano, Takashi and Otsuka, Makoto},
  date = {2011},
  journaltitle = {BMC Neuroscience},
  volume = {12},
  pages = {P244},
  doi = {10.1186/1471-2202-12-s1-p244},
  url = {https://doi.org/10.1186/1471-2202-12-s1-p244},
  date_added = {Thu Jan 16 23:13:33 2020},
  number = {S1}
}

@book{nasarGrandPursuitStory2011,
  title = {Grand Pursuit: {{The}} Story of Economic Genius},
  author = {Nasar, Sylvia},
  date = {2011},
  publisher = {{Simon \& Schuster}},
  file = {/home/jethro/Dropbox/Calibre/Sylvia Nasar/Grand Pursuit (213)/Grand Pursuit - Sylvia Nasar.epub},
  isbn = {0-684-87298-6 978-0-684-87298-8}
}

@article{nateliason_how_take_smart_notes,
  title = {How to Take Smart Notes: {{A}} Step-by-Step Guide - Nat Eliason},
  author = {Eliason, Nat},
  date = {2020},
  url = {https://www.nateliason.com/blog/smart-notes},
  note = {Online; accessed 14 February 2020}
}

@article{neftci19_surrog_gradien_learn_spikin_neural_networ,
  title = {Surrogate Gradient Learning in Spiking Neural Networks},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1901.09948v2},
  abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  archivePrefix = {arXiv},
  eprint = {1901.09948},
  eprinttype = {arxiv},
  primaryClass = {cs.NE}
}

@online{neftciSurrogateGradientLearning2019,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  date = {2019-05-03},
  url = {http://arxiv.org/abs/1901.09948},
  urldate = {2020-07-16},
  abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  archivePrefix = {arXiv},
  eprint = {1901.09948},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/UPQVP8FQ/Neftci et al. - 2019 - Surrogate Gradient Learning in Spiking Neural Netw.pdf;/home/jethro/Zotero/storage/ZR78LEXJ/1901.html},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  primaryClass = {cs, q-bio}
}

@article{newcombe15_how_amazon_web_servic_uses_formal_method,
  title = {How Amazon Web Services Uses Formal Methods},
  author = {Newcombe, Chris and Rath, Tim and Zhang, Fan and Munteanu, Bogdan and Brooker, Marc and Deardeuff, Michael},
  date = {2015},
  journaltitle = {Communications of the ACM},
  volume = {58},
  pages = {66--73},
  doi = {10.1145/2699417},
  url = {https://doi.org/10.1145/2699417},
  date_added = {Thu Jan 16 14:52:06 2020},
  number = {4}
}

@article{nilil_instal_ubunt_ros_wiki,
  title = {Melodic/{{Installation}}/{{Ubuntu}} - {{ROS}} Wiki},
  author = {{nil}},
  year = {nil},
  url = {http://wiki.ros.org/melodic/Installation/Ubuntu},
  note = {Online; accessed 16 October 2019}
}

@incollection{NIPS2016_6383,
  title = {Unifying Count-Based Exploration and Intrinsic Motivation},
  booktitle = {Advances in Neural Information Processing Systems 29},
  author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  date = {2016},
  pages = {1471--1479},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6383-unifying-count-based-exploration-and-intrinsic-motivation.pdf}
}

@incollection{NIPS2018_7415,
  title = {{{SLAYER}}: {{Spike}} Layer Error Reassignment in Time},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Shrestha, Sumit Bam and Orchard, Garrick},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {1412--1421},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf}
}

@article{openai_gym,
  title = {{{OpenAI}} Gym},
  author = {{OpenAI}},
  date = {2019},
  url = {https://gym.openai.com/envs/CartPole-v0/},
  note = {Online; accessed 02 November 2019}
}

@article{paine19_makin_effic_use_demon_to,
  title = {Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},
  author = {Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and Barth-Maron, Gabriel and Wang, Ziyu and de Freitas, Nando and Team, Worlds},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1909.01387v1},
  abstract = {This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.},
  archivePrefix = {arXiv},
  eprint = {1909.01387},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{papamakarios19_normal_flows_probab_model_infer,
  title = {Normalizing Flows for Probabilistic Modeling and Inference},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1912.02762v1},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/34IWCA3A/Papamakarios et al. - 2019 - Normalizing flows for probabilistic modeling and i.pdf},
  primaryClass = {stat.ML}
}

@inproceedings{parkPerformanceImprovementDeep2016,
  title = {Performance Improvement of Deep Learning Based Gesture Recognition Using Spatiotemporal Demosaicing Technique},
  booktitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Park, Paul K. J. and Cho, Baek Hwan and Park, Jin Man and Lee, Kyoobin and Kim, Ha Young and Kang, Hyo Ah and Lee, Hyun Goo and Woo, Jooyeon and Roh, Yohan and Lee, Won Jo and Shin, Chang-Woo and Wang, Qiang and Ryu, Hyunsurk},
  date = {2016-09},
  pages = {1624--1628},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2016.7532633},
  abstract = {We propose a novel method for the demosaicing of event-based images that offers substantial performance improvement of far-distance gesture recognition based on deep Convolutional Neural Network. Unlike the conventional demosaicing technique using the spatial color interpolation of Bayer patterns, our new approach utilizes spatiotemporal correlation between pixel arrays, whereby timestamps of high-resolution pixels are efficiently generated in real-time from the event data. In this paper, we describe this new method and evaluate its performance with a hand motion recognition task.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  file = {/home/jethro/Zotero/storage/ZZ2TXASQ/7532633.html},
  keywords = {Bayer patterns,Bayes methods,convolutional,convolutional neural network,Correlation,deep learning,demosaicing,Dynamic Vision Sensor,far distance gesture recognition,gesture recognition,hand motion recognition,high-resolution pixels,image motion analysis,Image recognition,image resolution,Image resolution,image segmentation,Interpolation,motion,neural nets,neural network,performance improvement,recognition,spatial color interpolation,spatiotemporal correlation,spatiotemporal demosaicing technique,Spatiotemporal phenomena,Thumb,Voltage control}
}

@article{pati17_statis_optim_variat_bayes,
  title = {On Statistical Optimality of Variational Bayes},
  author = {Pati, Debdeep and Bhattacharya, Anirban and Yang, Yun},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1712.08983v1},
  abstract = {The article addresses a long-standing open problem on the justification of using variational Bayes methods for parameter estimation. We provide general conditions for obtaining optimal risk bounds for point estimates acquired from mean-field variational Bayesian inference. The conditions pertain to the existence of certain test functions for the distance metric on the parameter space and minimal assumptions on the prior. A general recipe for verification of the conditions is outlined which is broadly applicable to existing Bayesian models with or without latent variables. As illustrations, specific applications to Latent Dirichlet Allocation and Gaussian mixture models are discussed.},
  archivePrefix = {arXiv},
  eprint = {1712.08983},
  eprinttype = {arxiv},
  primaryClass = {math.ST}
}

@online{PDFLearningHandeye,
  title = {[{{PDF}}] {{Learning}} Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection | {{Semantic Scholar}}},
  url = {/paper/Learning-hand-eye-coordination-for-robotic-grasping-Levine-Pastor/494e2d5b40dcebde349f9872c7317e5003f9c5d2},
  urldate = {2020-07-13},
  abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. We describe two large-scale experiments that we conducted on two separate robotic platforms. In the first experiment, about 800,000 grasp attempts were collected over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and gripper wear and tear. In the second experiment, we used a different robotic platform and 8 robots to collect a dataset consisting of over 900,000 grasp attempts. The second robotic platform was used to test transfer between robots, and the degree to which data from a different set of robots can be used to aid learning. Our experimental results demonstrate that our approach achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. Our transfer experiment also illustrates that data from different robots can be combined to learn more reliable and effective grasping.},
  file = {/home/jethro/Zotero/storage/QKFUUKS4/494e2d5b40dcebde349f9872c7317e5003f9c5d2.html},
  langid = {english}
}

@article{pfeiffer2018deep,
  title = {Deep Learning with Spiking Neurons: Opportunities and Challenges},
  author = {Pfeiffer, Michael and Pfeil, Thomas},
  date = {2018},
  journaltitle = {Frontiers in neuroscience},
  volume = {12},
  publisher = {{Frontiers Media SA}}
}

@book{pinsky2010introduction,
  title = {An Introduction to Stochastic Modeling},
  author = {Pinsky, Mark and Karlin, Samuel},
  date = {2010},
  publisher = {{Academic press}}
}

@inproceedings{pmlr-v38-srivastava15,
  title = {{{WASP}}: {{Scalable Bayes}} via Barycenters of Subset Posteriors},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  author = {Srivastava, Sanvesh and Cevher, Volkan and Dinh, Quoc and Dunson, David},
  editor = {Lebanon, Guy and Vishwanathan, S. V. N.},
  date = {2015-05-09/2015-05-12},
  volume = {38},
  pages = {912--920},
  publisher = {{PMLR}},
  location = {{San Diego, California, USA}},
  url = {http://proceedings.mlr.press/v38/srivastava15.html},
  abstract = {The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on different machines in a distributed manner. We propose a simple, general, and highly efficient approach, which first runs a posterior sampling algorithm in parallel on different machines for subsets of a large data set. To combine these subset posteriors, we calculate the Wasserstein barycenter via a highly efficient linear program. The resulting estimate for the Wasserstein posterior (WASP) has an atomic form, facilitating straightforward estimation of posterior summaries of functionals of interest. The WASP approach allows posterior sampling algorithms for smaller data sets to be trivially scaled to huge data. We provide theoretical justification in terms of posterior consistency and algorithm efficiency. Examples are provided in complex settings including Gaussian process regression and nonparametric Bayes mixture models.},
  pdf = {http://proceedings.mlr.press/v38/srivastava15.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v70-haarnoja17a,
  title = {Reinforcement Learning with Deep Energy-Based Policies},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  editor = {Precup, Doina and Teh, Yee Whye},
  date = {2017-08-06/2017-08-11},
  volume = {70},
  pages = {1352--1361},
  publisher = {{PMLR}},
  location = {{International Convention Centre, Sydney, Australia}},
  url = {http://proceedings.mlr.press/v70/haarnoja17a.html},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  pdf = {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v89-o-connor19a,
  title = {Training a Spiking Neural Network with Equilibrium Propagation},
  booktitle = {Proceedings of Machine Learning Research},
  author = {O'Connor, Peter and Gavves, Efstratios and Welling, Max},
  editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  date = {2019-04-16/2019-04-18},
  volume = {89},
  pages = {1516--1523},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v89/o-connor19a.html},
  abstract = {Backpropagation is almost universally used to train artificial neural networks. However, there are several reasons that backpropagation could not be plausibly implemented by biological neurons. Among these are the facts that (1) biological neurons appear to lack any mechanism for sending gradients backwards across synapses, and (2) biological “spiking” neurons emit binary signals, whereas back-propagation requires that neurons communicate continuous values between one another. Recently, Scellier and Bengio [2017], demonstrated an alternative to backpropagation, called Equilibrium Propagation, wherein gradients are implicitly computed by the dynamics of the neural network, so that neurons do not need an internal mechanism for backpropagation of gradients. This provides an interesting solution to problem (1). In this paper, we address problem (2) by proposing a way in which Equilibrium Propagation can be implemented with neurons which are constrained to just communicate binary values at each time step. We show that with appropriate step-size annealing, we can converge to the same fixed-point as a real-valued neural network, and that with predictive coding, we can make this convergence much faster. We demonstrate that the resulting model can be used to train a spiking neural network using the update scheme from Equilibrium propagation.},
  pdf = {http://proceedings.mlr.press/v89/o-connor19a/o-connor19a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{pong19_skew_fit,
  title = {Skew-Fit: {{State}}-Covering Self-Supervised Reinforcement Learning},
  author = {Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1903.03698v2},
  abstract = {In standard reinforcement learning, each new skill requires a manually-designed reward function, which takes considerable manual effort and engineering. Self-supervised goal setting has the potential to automate this process, enabling an agent to propose its own goals and acquire skills that achieve these goals. However, such methods typically rely on manually-designed goal distributions, or heuristics to force the agent to explore a wide range of states. We propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing the entropy of the goal distribution together with goal reaching performance, where goals correspond to entire states. We present an algorithm called Skew-Fit for learning such a maximum-entropy goal distribution, and show that under certain regularity conditions, our method converges to a uniform distribution over the set of possible states, even when we do not know this set beforehand. Skew-Fit enables self-supervised agents to autonomously choose and practice diverse goals. Our experiments show that it can learn a variety of manipulation tasks from images, including opening a door with a real robot, entirely from scratch and without any manually-designed reward function.},
  archivePrefix = {arXiv},
  eprint = {1903.03698},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{poole19_variat_bound_mutual_infor,
  title = {On Variational Bounds of Mutual Information},
  author = {Poole, Ben and Ozair, Sherjil and van den Oord, Aaron and Alemi, Alexander A. and Tucker, George},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1905.06922v1},
  abstract = {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.},
  archivePrefix = {arXiv},
  eprint = {1905.06922v1},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{queiroz06_reinf_learn_simpl_contr_task,
  title = {Reinforcement Learning of a Simple Control Task Using the Spike Response Model},
  author = {de Queiroz, Murilo Saraiva and de Berrêdo, Roberto Coelho and de Pádua Braga, Antônio},
  date = {2006},
  journaltitle = {Neurocomputing},
  volume = {70},
  pages = {14--20},
  doi = {10.1016/j.neucom.2006.07.002},
  url = {https://doi.org/10.1016/j.neucom.2006.07.002},
  date_added = {Thu Aug 29 12:45:08 2019},
  number = {1-3},
  options = {useprefix=true}
}

@article{rackauckas19_diffeq,
  title = {Diffeqflux.Jl - a Julia Library for Neural Differential Equations},
  author = {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and Bettencourt, Jesse and White, Lyndon and Dixit, Vaibhav},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1902.02376v1},
  abstract = {DiffEqFlux.jl is a library for fusing neural networks and differential equations. In this work we describe differential equations from the viewpoint of data science and discuss the complementary nature between machine learning models and differential equations. We demonstrate the ability to incorporate DifferentialEquations.jl-defined differential equation problems into a Flux-defined neural network, and vice versa. The advantages of being able to use the entire DifferentialEquations.jl suite for this purpose is demonstrated by counter examples where simple integration strategies fail, but the sophisticated integration strategies provided by the DifferentialEquations.jl library succeed. This is followed by a demonstration of delay differential equations and stochastic differential equations inside of neural networks. We show high-level functionality for defining neural ordinary differential equations (neural networks embedded into the differential equation) and describe the extra models in the Flux model zoo which includes neural stochastic differential equations. We conclude by discussing the various adjoint methods used for backpropogation of the differential equation solvers. DiffEqFlux.jl is an important contribution to the area, as it allows the full weight of the differential equation solvers developed from decades of research in the scientific computing field to be readily applied to the challenges posed by machine learning and data science.},
  archivePrefix = {arXiv},
  eprint = {1902.02376},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{Rafferty_2015,
  title = {Faster Teaching via {{POMDP}} Planning},
  author = {Rafferty, Anna N. and Brunskill, Emma and Griffiths, Thomas L. and Shafto, Patrick},
  date = {2015-09},
  journaltitle = {Cognitive Science},
  volume = {40},
  pages = {1290--1332},
  publisher = {{Wiley}},
  issn = {0364-0213},
  doi = {10.1111/cogs.12290},
  url = {http://dx.doi.org/10.1111/cogs.12290},
  number = {6}
}

@online{RAIICppreferenceCom,
  title = {{{RAII}} - Cppreference.Com},
  url = {https://en.cppreference.com/w/cpp/language/raii},
  urldate = {2020-06-24},
  file = {/home/jethro/Zotero/storage/TSGIG823/raii.html}
}

@article{ranganath13_black_box_variat_infer,
  title = {Black Box Variational Inference},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
  date = {2013},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1401.0118v1},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  archivePrefix = {arXiv},
  eprint = {1401.0118},
  eprinttype = {arxiv},
  primaryClass = {stat.ML}
}

@inproceedings{ratliff2006maximum,
  title = {Maximum Margin Planning},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  date = {2006},
  pages = {729--736},
  organization = {{ACM}}
}

@article{ravi_bayesian_teaching_mnist,
  title = {Bayesian Teaching as Model Explanation: {{An MNIST}} Example},
  author = {Sojitra, Ravi},
  date = {2018},
  url = {https://ravisoji.com/2018/03/04/bayesian-teaching-as-explanation.html},
  note = {Online; accessed 19 May 2019}
}

@inproceedings{rebecqEventsToVideoBringingModern2019,
  title = {Events-{{To}}-{{Video}}: {{Bringing Modern Computer Vision}} to {{Event Cameras}}},
  shorttitle = {Events-{{To}}-{{Video}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rebecq, Henri and Ranftl, Rene and Koltun, Vladlen and Scaramuzza, Davide},
  date = {2019-06},
  pages = {3852--3861},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00398},
  url = {https://ieeexplore.ieee.org/document/8953722/},
  urldate = {2020-06-30},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/home/jethro/Zotero/storage/7T7VT24K/rebecq2019.pdf;/home/jethro/Zotero/storage/8MRJZ8YL/Rebecq et al. - 2019 - Events-To-Video Bringing Modern Computer Vision t.pdf},
  isbn = {978-1-72813-293-8},
  langid = {english}
}

@article{richardson06_markov_logic_networ,
  title = {Markov Logic Networks},
  author = {Richardson, Matthew and Domingos, Pedro},
  date = {2006},
  journaltitle = {Machine Learning},
  volume = {62},
  pages = {107--136},
  doi = {10.1007/s10994-006-5833-1},
  url = {https://doi.org/10.1007/s10994-006-5833-1},
  date_added = {Wed Jul 24 22:57:34 2019},
  number = {1-2}
}

@book{ross2014introduction,
  title = {Introduction to Probability Models},
  author = {Ross, Sheldon M},
  date = {2014},
  publisher = {{Academic press}}
}

@article{rueckauer16_theor_tools_conver_analog_to,
  title = {Theory and Tools for the Conversion of Analog to Spiking Convolutional Neural Networks},
  author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael},
  date = {2016},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1612.04052v1},
  abstract = {Deep convolutional neural networks (CNNs) have shown great potential for numerous real-world machine learning applications, but performing inference in large CNNs in real-time remains a challenge. We have previously demonstrated that traditional CNNs can be converted into deep spiking neural networks (SNNs), which exhibit similar accuracy while reducing both latency and computational load as a consequence of their data-driven, event-based style of computing. Here we provide a novel theory that explains why this conversion is successful, and derive from it several new tools to convert a larger and more powerful class of deep networks into SNNs. We identify the main sources of approximation errors in previous conversion methods, and propose simple mechanisms to fix these issues. Furthermore, we develop spiking implementations of common CNN operations such as max-pooling, softmax, and batch-normalization, which allow almost loss-less conversion of arbitrary CNN architectures into the spiking domain. Empirical evaluation of different network architectures on the MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.},
  archivePrefix = {arXiv},
  eprint = {1612.04052},
  eprinttype = {arxiv},
  primaryClass = {stat.ML}
}

@article{ruiz19_contr_diver_combin_variat_infer_mcmc,
  title = {A Contrastive Divergence for Combining Variational Inference and Mcmc},
  author = {Ruiz, Francisco J. R. and Titsias, Michalis K.},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1905.04062v1},
  abstract = {We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).},
  archivePrefix = {arXiv},
  eprint = {1905.04062},
  eprinttype = {arxiv},
  primaryClass = {stat.ML}
}

@article{rusu15_polic_distil,
  title = {Policy Distillation},
  author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  date = {2015},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1511.06295v2},
  abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
  archivePrefix = {arXiv},
  eprint = {1511.06295},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{sallans04a_ferl,
  title = {Reinforcement Learning with Factored States and Actions.},
  author = {Sallans, Brian and Hinton, Geoffrey},
  date = {2004-08},
  journaltitle = {Journal of Machine Learning Research},
  volume = {5},
  pages = {1063--1088}
}

@article{sboev18_spikin_neural_networ_reinf_learn,
  title = {Spiking Neural Network Reinforcement Learning Method Based on Temporal Coding and Stdp},
  author = {Sboev, Alexander and Vlasov, Danila and Rybka, Roman and Serenko, Alexey},
  date = {2018},
  journaltitle = {Procedia Computer Science},
  volume = {145},
  pages = {458--463},
  doi = {10.1016/j.procs.2018.11.107},
  url = {https://doi.org/10.1016/j.procs.2018.11.107},
  date_added = {Mon Sep 30 11:08:34 2019},
  issue = {nil}
}

@article{schaul15_prior_exper_replay,
  title = {Prioritized Experience Replay},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date = {2015},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1511.05952v4},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archivePrefix = {arXiv},
  eprint = {1511.05952},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{schliebs13_evolv_spikin_neural_networ_survey,
  title = {Evolving Spiking Neural Network-a Survey},
  author = {Schliebs, Stefan and Kasabov, Nikola},
  date = {2013},
  journaltitle = {Evolving Systems},
  volume = {4},
  pages = {87--98},
  doi = {10.1007/s12530-013-9074-9},
  url = {https://doi.org/10.1007/s12530-013-9074-9},
  date_added = {Mon Jan 6 18:15:22 2020},
  number = {2}
}

@online{schmarjeSurveySemiSelf2020,
  title = {A Survey on {{Semi}}-, {{Self}}- and {{Unsupervised Learning}} in {{Image Classification}}},
  author = {Schmarje, Lars and Santarossa, Monty and Schröder, Simon-Martin and Koch, Reinhard},
  date = {2020-06-26},
  url = {http://arxiv.org/abs/2002.08721},
  urldate = {2020-07-11},
  abstract = {While deep learning strategies achieve outstanding results in computer vision tasks, one issue remains: The current strategies rely heavily on a huge amount of labeled data. In many real-world problems, it is not feasible to create such an amount of labeled training data. Therefore, it is common to incorporate unlabeled data into the training process to reach equal results with fewer labels. Due to a lot of concurrent research, it is difficult to keep track of recent developments. In this survey, we provide an overview of often used ideas and methods in image classification with fewer labels. We compare 25 methods. In our analysis, we identify three major trends. 1. State-of-theart methods are scaleable to real-world applications based on their accuracy. 2. The degree of supervision which is needed to achieve comparable results to the usage of all labels is decreasing. 3. All methods share common ideas while only a few methods combine these ideas to achieve better performance. Based on all of these three trends we discover future research opportunities.},
  archivePrefix = {arXiv},
  eprint = {2002.08721},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/PURYDN82/Schmarje et al. - 2020 - A survey on Semi-, Self- and Unsupervised Learning.pdf;/home/jethro/Zotero/storage/5TSESXVX/2002.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{schulman15_high_dimen_contin_contr_using,
  title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  date = {2015},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1506.02438v6},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archivePrefix = {arXiv},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{schultz97_neural_subst_predic_rewar,
  title = {A Neural Substrate of Prediction and Reward},
  author = {Schultz, W. and Dayan, P. and Montague, P. R.},
  date = {1997},
  journaltitle = {Science},
  volume = {275},
  pages = {1593--1599},
  doi = {10.1126/science.275.5306.1593},
  url = {https://doi.org/10.1126/science.275.5306.1593},
  date_added = {Sun Dec 15 13:23:11 2019},
  number = {5306}
}

@inproceedings{sermanetTimeContrastiveNetworksSelfSupervised2018,
  title = {Time-{{Contrastive Networks}}: {{Self}}-{{Supervised Learning}} from {{Video}}},
  shorttitle = {Time-{{Contrastive Networks}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  date = {2018-05},
  pages = {1134--1141},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8462891},
  abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  file = {/home/jethro/Zotero/storage/5RBZHS2Y/Sermanet et al. - 2018 - Time-Contrastive Networks Self-Supervised Learnin.pdf;/home/jethro/Zotero/storage/EIHPYWD6/8462891.html},
  keywords = {end-effectors,human poses,image representation,learning (artificial intelligence),Learning (artificial intelligence),Lighting,Liquids,pose estimation,reinforcement learning algorithm,robot programming,robot vision,robotic behaviors,robotic imitation settings,robotic systems,Robots,self-supervised learning,Task analysis,time-contrastive networks,Training,video signal processing,viewpoint-invariant representation,Visualization}
}

@article{Severa2016SpikingNA,
  title = {Spiking Network Algorithms for Scientific Computing},
  author = {Severa, William and Parekh, Ojas and Carlson, Kristofor D. and James, Conrad D. and Aimone, James B.},
  date = {2016},
  journaltitle = {2016 IEEE International Conference on Rebooting Computing (ICRC)},
  pages = {1--8}
}

@book{shalev2014understanding,
  ids = {Shalev-Shwartz:2014:UML:2621980},
  title = {Understanding Machine Learning: {{From}} Theory to Algorithms},
  author = {Shalev-Shwartz, Shai and Ben-David, Shai},
  date = {2014},
  publisher = {{Cambridge university press}}
}

@article{SHRESTHA201733,
  title = {Robust Spike-Train Learning in Spike-Event Based Weight Update},
  author = {Shrestha, Sumit Bam and Song, Qing},
  date = {2017},
  journaltitle = {Neural Networks},
  volume = {96},
  pages = {33--46},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2017.08.010},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608017302009},
  abstract = {Supervised learning algorithms in a spiking neural network either learn a spike-train pattern for a single neuron receiving input spike-train from multiple input synapses or learn to output the first spike time in a feedforward network setting. In this paper, we build upon spike-event based weight update strategy to learn continuous spike-train in a spiking neural network with a hidden layer using a dead zone on–off based adaptive learning rate rule which ensures convergence of the learning process in the sense of weight convergence and robustness of the learning process to external disturbances. Based on different benchmark problems, we compare this new method with other relevant spike-train learning algorithms. The results show that the speed of learning is much improved and the rate of successful learning is also greatly improved.},
  keywords = {Adaptive learning rate,Multilayer spike-train learning,Robust stability,Spiking neural network,Supervised learning,Weight convergence}
}

@article{shwartz-ziv17_openin_black_box_deep_neural,
  title = {Opening the Black Box of Deep Neural Networks via Information},
  author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1703.00810v3},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \emph{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \emph{c}ompression of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  archivePrefix = {arXiv},
  eprint = {1703.00810},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@article{simard17_machin_teach,
  title = {Machine Teaching: A New Paradigm for Building Machine Learning Systems},
  author = {Simard, Patrice Y. and Amershi, Saleema and Chickering, David M. and Pelton, Alicia Edelman and Ghorashi, Soroush and Meek, Christopher and Ramos, Gonzalo and Suh, Jina and Verwey, Johan and Wang, Mo and Wernsing, John},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1707.06742v3},
  abstract = {The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. While machine learning focuses on creating new algorithms and improving the accuracy of "learners", the machine teaching discipline focuses on the efficacy of the "teachers". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.},
  archivePrefix = {arXiv},
  eprint = {1707.06742},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@inproceedings{smith_quoc_bayes_generalization_sgd,
  title = {A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
  author = {Smith, Sam and Le, Quoc V.},
  date = {2018},
  url = {https://openreview.net/pdf?id=BJij4yg0Z}
}

@article{SnnSlam,
  title = {Spiking Neural Network on Neuromorphic Hardware for Energy-Efficient Unidimensional Slam},
  author = {Tang, Guangzhi and Shah, Arpit and Michmizos, Konstantinos P.},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1903.02504v2},
  abstract = {Energy-efficient simultaneous localization and mapping (SLAM) is crucial for mobile robots exploring unknown environments. The mammalian brain solves SLAM via a network of specialized neurons, exhibiting asynchronous computations and event-based communications, with very low energy consumption. We propose a brain-inspired spiking neural network (SNN) architecture that solves the unidimensional SLAM by introducing spike-based reference frame transformation, visual likelihood computation, and Bayesian inference. We integrated our neuromorphic algorithm to Intel's Loihi neuromorphic processor, a non-Von Neumann hardware that mimics the brain's computing paradigms. We performed comparative analyses for accuracy and energy-efficiency between our neuromorphic approach and the GMapping algorithm, which is widely used in small environments. Our Loihi-based SNN architecture consumes 100 times less energy than GMapping run on a CPU while having comparable accuracy in head direction localization and map-generation. These results pave the way for scaling our approach towards active-SLAM alternative solutions for Loihi-controlled autonomous robots.},
  archivePrefix = {arXiv},
  eprint = {1903.02504},
  eprinttype = {arxiv},
  primaryClass = {cs.RO}
}

@article{so_why_move_shared_ptr,
  title = {C++ - {{Why}} Would {{I}} Std::Move an Std::Sharedₚtr? - {{Stack Overflow}}},
  author = {{nil}},
  date = {2019},
  url = {https://stackoverflow.com/questions/41871115/why-would-i-stdmove-an-stdshared-ptr},
  note = {Online; accessed 25 February 2019}
}

@inproceedings{spikeprop,
  title = {{{SpikeProp}}: Backpropagation for Networks of Spiking Neurons.},
  author = {Bohte, Sander and Kok, Joost and Poutré, Johannes},
  date = {2000-01},
  volume = {48},
  pages = {419--424},
  series = {{{ESANN}}}
}

@inproceedings{stagstedNeuromorphicControlSpiking2020,
  title = {Towards Neuromorphic Control: {{A}} Spiking Neural Network Based {{PID}} Controller for {{UAV}}},
  shorttitle = {Towards Neuromorphic Control},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Stagsted, Rasmus and Vitale, Antonio and Binz, Jonas and Renner, Alpha and Bonde Larsen, Leon and Sandamirskaya, Yulia},
  date = {2020-07-12},
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2020.XVI.074},
  url = {http://www.roboticsproceedings.org/rss16/p074.pdf},
  urldate = {2020-07-11},
  abstract = {In this work, we present a spiking neural network (SNN) based PID controller on a neuromorphic chip. On-chip SNNs are currently being explored in low-power AI applications. Due to potentially ultra-low power consumption, low latency, and high processing speed, on-chip SNNs are a promising tool for control of power-constrained platforms, such as Unmanned Aerial Vehicles (UAV). To obtain highly efficient and fast end-toend neuromorphic controllers, the SNN-based AI architectures must be seamlessly integrated with motor control. Towards this goal, we present here the first implementation of a fully neuromorphic PID controller. We interfaced Intel’s neuromorphic research chip Loihi to a UAV, constrained to a single degree of freedom. We developed an SNN control architecture using populations of spiking neurons, in which each spike carries information about the measured, control, or error value, defined by the identity of the spiking neuron. Using this sparse code, we realize a precise PID controller. The P, I, and D gains of the controller are implemented as synaptic weights that can adapt according to an on-chip plasticity rule. In future work, these plastic synapses can be used to tune and adapt the controller autonomously.},
  eventtitle = {Robotics: {{Science}} and {{Systems}} 2020},
  file = {/home/jethro/Zotero/storage/EUTEP7ES/Stagsted et al. - 2020 - Towards neuromorphic control A spiking neural net.pdf},
  isbn = {978-0-9923747-6-1},
  langid = {english}
}

@article{stemmler96_singl_spike_suffic,
  title = {A Single Spike Suffices: The Simplest Form of Stochastic Resonance in Model Neurons},
  author = {Stemmler, Martin},
  date = {1996},
  journaltitle = {Network: Computation in Neural Systems},
  volume = {7},
  pages = {687--716},
  doi = {10.1088/0954-898x_7_4_005},
  url = {https://doi.org/10.1088/0954-898x_7_4_005},
  date_added = {Sat Nov 2 19:32:20 2019},
  number = {4}
}

@article{stock19_and_bit_goes_down,
  title = {And the Bit Goes down: {{Revisiting}} the Quantization of Neural Networks},
  author = {Stock, Pierre and Joulin, Armand and Gribonval, Rémi and Graham, Benjamin and Jégou, Hervé},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1907.05686v2},
  abstract = {In this paper, we address the problem of reducing the memory footprint of ResNet-like convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs and not its weights. The advantage of our approach is that it minimizes the loss reconstruction error for in-domain inputs and does not require any labelled data. We also use byte-aligned codebooks to produce compressed networks with efficient inference on CPU. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB (20x compression factor) while preserving a top-1 accuracy of 76.1 \% on ImageNet object classification and by compressing a Mask R-CNN with a size budget around 6 MB.},
  archivePrefix = {arXiv},
  eprint = {1907.05686},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@inproceedings{stolle2002learning,
  title = {Learning Options in Reinforcement Learning},
  booktitle = {International {{Symposium}} on Abstraction, Reformulation, and Approximation},
  author = {Stolle, Martin and Precup, Doina},
  date = {2002},
  pages = {212--223},
  organization = {{Springer}}
}

@inproceedings{sutton2000policy,
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  date = {2000},
  pages = {1057--1063}
}

@inproceedings{systemsengineering_6575245,
  title = {Systems Engineering the Curiosity Rover: {{A}} Retrospective},
  booktitle = {2013 8th International Conference on System of Systems Engineering},
  author = {Welch, R. and Limonadi, D. and Manning, R.},
  date = {2013-06},
  pages = {70--75},
  doi = {10.1109/SYSoSE.2013.6575245},
  keywords = {Actuators,Aerospace electronics,Complexity theory,Computer architecture,cruise system,descent and landing (spacecraft),Earth,entry,entry descent and landing system,Instruments,Mars,Mars Science Laboratory Curiosity Rover,Mars surface,MSL,planetary rovers,planetary surfaces,Robotics,rover complex science payload,Rovers,scientific exploration,Software,spacecraft,systems engineering,Systems Engineering,systems sampling system}
}

@article{TAVANAEI201947,
  title = {Deep Learning in Spiking Neural Networks},
  author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timothée and Maida, Anthony},
  date = {2019},
  journaltitle = {Neural Networks},
  volume = {111},
  pages = {47--63},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.12.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608018303332},
  abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
  keywords = {Biological plausibility,Deep learning,Machine learning,Power-efficient architecture,Spiking neural network}
}

@online{tayEfficientTransformersSurvey2020,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  date = {2020-09-16},
  url = {http://arxiv.org/abs/2009.06732},
  urldate = {2020-10-27},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archivePrefix = {arXiv},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning,NLP},
  primaryClass = {cs}
}

@article{tensorflow_stand_keras,
  title = {Standardizing on Keras: {{Guidance}} on High-Level {{APIs}} in {{TensorFlow}} 2.0},
  author = {{Tensorflow}},
  date = {2018},
  url = {https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a},
  note = {Online; accessed 07 January 2019}
}

@book{thrun2005probabilistic,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  date = {2005},
  publisher = {{MIT press}}
}

@article{titsias18_unbias_implic_variat_infer,
  title = {Unbiased Implicit Variational Inference},
  author = {Titsias, Michalis K. and Ruiz, Francisco J. R.},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1808.02078v3},
  abstract = {We develop unbiased implicit variational inference (UIVI), a method that expands the applicability of variational inference by defining an expressive variational family. UIVI considers an implicit variational distribution obtained in a hierarchical manner using a simple reparameterizable distribution whose variational parameters are defined by arbitrarily flexible deep neural networks. Unlike previous works, UIVI directly optimizes the evidence lower bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on several models, including Bayesian multinomial logistic regression and variational autoencoders, and show that UIVI achieves both tighter ELBO and better predictive performance than existing approaches at a similar computational cost.},
  archivePrefix = {arXiv},
  eprint = {1808.02078},
  eprinttype = {arxiv},
  primaryClass = {stat.ML}
}

@article{trimstray_se_hackers,
  title = {Trimstray on {{Twitter}}: \&quot;{{Search Engines}} for {{Hackers}}:\&\#10;\&\#10;{{https://t.co/Awr3X88Xu1\&\#10;https://t.co/03trsWUrnP\&\#10;https://t.co/B9IHX23MeC\&\#10;https://t.co/uO1oFjB7Eb\&\#10;https://t.co/NE7FSOQsPl\&\#10;https://t.co/s2wG7cOGa5\&\#10;https://t.co/uBqtz7QuUD\&\#10;https://t.co/IZx4B82wLQ\&\#10;https://t.co/Oa04GvDxTp\&\#10;https://t.co/TKjuUVU9il\&\#10;\&\#10;\#it}} \#tech\&quot;},
  author = {{trimstray}},
  date = {2019},
  url = {https://twitter.com/trimstray/status/1086705742793658369},
  note = {Online; accessed 09 February 2019}
}

@online{tschannenMutualInformationMaximization2020,
  title = {On {{Mutual Information Maximization}} for {{Representation Learning}}},
  author = {Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K. and Gelly, Sylvain and Lucic, Mario},
  date = {2020-01-23},
  url = {http://arxiv.org/abs/1907.13625},
  urldate = {2020-07-08},
  abstract = {Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.},
  archivePrefix = {arXiv},
  eprint = {1907.13625},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/IPBXIGEA/Tschannen et al. - 2020 - On Mutual Information Maximization for Representat.pdf;/home/jethro/Zotero/storage/DEVRXWG3/1907.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICLR 2020. Michael Tschannen and Josip Djolonga contributed equally},
  primaryClass = {cs, stat}
}

@article{tschiatschek19_learn_aware_teach,
  title = {Learner-Aware Teaching: {{Inverse}} Reinforcement Learning with Preferences and Constraints},
  author = {Tschiatschek, Sebastian and Ghosh, Ahana and Haug, Luis and Devidze, Rati and Singla, Adish},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1906.00429v1},
  abstract = {Inverse reinforcement learning (IRL) enables an agent to learn complex behavior by observing demonstrations from a (near-)optimal policy. The typical assumption is that the learner's goal is to match the teacher's demonstrated behavior. In this paper, we consider the setting where the learner has her own preferences that she additionally takes into consideration. These preferences can for example capture behavioral biases, mismatched worldviews, or physical constraints. We study two teaching approaches: learner-agnostic teaching, where the teacher provides demonstrations from an optimal policy ignoring the learner's preferences, and learner-aware teaching, where the teacher accounts for the learner's preferences. We design learner-aware teaching algorithms and show that significant performance improvements can be achieved over learner-agnostic teaching.},
  archivePrefix = {arXiv},
  eprint = {1906.00429},
  eprinttype = {arxiv},
  primaryClass = {cs.LG}
}

@online{UnsupervisedLearningDense,
  title = {Unsupervised {{Learning}} of {{Dense Optical Flow}}, {{Depth}} and {{Egomotion}} from {{Sparse Event Data}} | {{Semantic Scholar}}},
  url = {/paper/Unsupervised-Learning-of-Dense-Optical-Flow%2C-Depth-Ye-Mitrokhin/c140b2bd31b7b935ffc55340592d4429ca8d2a5e},
  urldate = {2020-07-07},
  abstract = {In this work we present a lightweight, unsupervised learning pipeline for \textbackslash textit\{dense\} depth, optical flow and egomotion estimation from sparse event output of the Dynamic Vision Sensor (DVS). To tackle this low level vision task, we use a novel encoder-decoder neural network architecture - ECN.  Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Due to the lightweight design, the inference part of the network runs at 250 FPS on a single GPU, making the pipeline ready for realtime robotics applications. Our experiments demonstrate significant improvements upon previous works that used deep learning on event data, as well as the ability of our pipeline to perform well during both day and night.},
  file = {/home/jethro/Zotero/storage/U4S4X479/c140b2bd31b7b935ffc55340592d4429ca8d2a5e.html},
  langid = {english}
}

@article{urbanczik09_gradien_learn_rule_tempot,
  title = {A Gradient Learning Rule for the Tempotron},
  author = {Urbanczik, Robert and Senn, Walter},
  date = {2009},
  journaltitle = {Neural Computation},
  volume = {21},
  pages = {340--352},
  doi = {10.1162/neco.2008.09-07-605},
  url = {https://doi.org/10.1162/neco.2008.09-07-605},
  date_added = {Fri Nov 1 16:00:33 2019},
  number = {2}
}

@inproceedings{van2016deep,
  title = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {Thirtieth {{AAAI}} Conference on Artificial Intelligence},
  author = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  date = {2016}
}

@online{venessGatedLinearNetworks2020,
  title = {Gated {{Linear Networks}}},
  author = {Veness, Joel and Lattimore, Tor and Budden, David and Bhoopchand, Avishkar and Mattern, Christopher and Grabska-Barwinska, Agnieszka and Sezener, Eren and Wang, Jianan and Toth, Peter and Schmitt, Simon and Hutter, Marcus},
  date = {2020-06-11},
  url = {http://arxiv.org/abs/1910.01526},
  urldate = {2020-06-16},
  abstract = {This paper presents a new family of backpropagation-free neural architectures, Gated Linear Networks (GLNs). What distinguishes GLNs from contemporary neural networks is the distributed and local nature of their credit assignment mechanism; each neuron directly predicts the target, forgoing the ability to learn feature representations in favor of rapid online learning. Individual neurons can model nonlinear functions via the use of data-dependent gating in conjunction with online convex optimization. We show that this architecture gives rise to universal learning capabilities in the limit, with effective model capacity increasing as a function of network size in a manner comparable with deep ReLU networks. Furthermore, we demonstrate that the GLN learning mechanism possesses extraordinary resilience to catastrophic forgetting, performing comparably to a MLP with dropout and Elastic Weight Consolidation on standard benchmarks. These desirable theoretical and empirical properties position GLNs as a complementary technique to contemporary offline deep learning methods.},
  archivePrefix = {arXiv},
  eprint = {1910.01526},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/HXI422IN/Veness et al. - 2020 - Gated Linear Networks.pdf;/home/jethro/Zotero/storage/Y5JMBIRU/1910.html},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: arXiv admin note: substantial text overlap with arXiv:1712.01897},
  primaryClass = {cs, math, stat}
}

@article{victor2005spike,
  title = {Spike Train Metrics},
  author = {Victor, Jonathan D},
  date = {2005},
  journaltitle = {Current opinion in neurobiology},
  volume = {15},
  pages = {585--592},
  publisher = {{Elsevier}},
  number = {5}
}

@article{VITANZA20153122,
  title = {Spiking Neural Controllers in Multi-Agent Competitive Systems for Adaptive Targeted Motor Learning},
  author = {Vitanza, Alessandra and Patané, Luca and Arena, Paolo},
  date = {2015},
  journaltitle = {Journal of the Franklin Institute},
  volume = {352},
  pages = {3122--3143},
  issn = {0016-0032},
  doi = {10.1016/j.jfranklin.2015.04.014},
  url = {http://www.sciencedirect.com/science/article/pii/S001600321500174X},
  abstract = {The proposed work introduces a neural control strategy for guiding adaptation in spiking neural structures acting as nonlinear controllers in a group of bio-inspired robots which compete in reaching targets in a virtual environment. The neural structures embedded into each agent are inspired by a specific part of the insect brain, namely Central Complex, devoted to detect, learn and memorize visual features for targeted motor control. A reduced-order model of a spiking neuron is used as the basic building block for the neural controller. The control methodology employs bio-inspired, correlation based learning mechanisms like Spike timing dependent plasticity with the addition of a reward/punishment-based method experimentally found in insects. The reference signal for the overall multi-agent control system is imposed by a global reward, which guides motor learning to direct each agent towards specific visual targets. The neural controllers within the agents start from identical conditions: the learning strategy induces each robot to show anticipated targeting actions upon specific visual stimuli. The whole control structure also contributes to make the robots refractory or more sensitive to specific visual stimuli, showing distinct preferences in future choices. This leads to an environmentally induced, targeted motor control, even without a direct communication among the agents, giving robots, while running, the ability to perform adaptation in real-time. Experiments, carried out in a dynamic simulation environment, show the suitability of the proposed approach. Specific performance indexes, like Shannon׳s Entropy, are adopted to quantitatively analyze diversity and specialization within the group.},
  note = {Special Issue on Advances in Nonlinear Dynamics and Control},
  number = {8}
}

@article{wang18_dkn,
  title = {Dkn: {{Deep}} Knowledge-Aware Network for News Recommendation},
  author = {Wang, Hongwei and Zhang, Fuzheng and Xie, Xing and Guo, Minyi},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1801.08284v2},
  abstract = {Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.},
  archivePrefix = {arXiv},
  eprint = {1801.08284},
  eprinttype = {arxiv},
  primaryClass = {stat.ML}
}

@book{White:2009:HDG:1717298,
  title = {Hadoop: {{The}} Definitive Guide},
  author = {White, Tom},
  date = {2009},
  edition = {1st},
  publisher = {{O'Reilly Media, Inc.}},
  isbn = {0-596-52197-9 978-0-596-52197-4}
}

@inproceedings{Whiteside2017HowTW,
  title = {How to Write a Technical Paper},
  author = {Whiteside, James D.},
  date = {2017}
}

@article{whittington19_theor_error_back_propag_brain,
  title = {Theories of Error Back-Propagation in the Brain},
  author = {Whittington, James C.R. and Bogacz, Rafal},
  date = {2019},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {23},
  pages = {235--250},
  doi = {10.1016/j.tics.2018.12.005},
  url = {https://doi.org/10.1016/j.tics.2018.12.005},
  date_added = {Tue Aug 20 10:09:27 2019},
  number = {3}
}

@article{wilson2019bayesian,
  title = {The Case for {{Bayesian}} Deep Learning},
  author = {Wilson, Andrew Gordon},
  date = {2019},
  journaltitle = {NYU Courant Technical Report},
  note = {Accessible at https://cims.nyu.edu/\textasciitilde andrewgw/caseforbdl.pdf}
}

@article{woodward19_learn_to_inter_learn_assis,
  title = {Learning to Interactively Learn and Assist},
  author = {Woodward, Mark and Finn, Chelsea and Hausman, Karol},
  date = {2019},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1906.10187v2},
  abstract = {When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone.},
  archivePrefix = {arXiv},
  eprint = {1906.10187},
  eprinttype = {arxiv},
  primaryClass = {cs.AI}
}

@online{wuStochasticNormalizingFlows2020,
  title = {Stochastic {{Normalizing Flows}}},
  author = {Wu, Hao and Köhler, Jonas and Noé, Frank},
  date = {2020-02-19},
  url = {http://arxiv.org/abs/2002.06707},
  urldate = {2020-07-19},
  abstract = {Normalizing flows are popular generative learning methods that train an invertible function to transform a simple prior distribution into a complicated target distribution. Here we generalize the framework by introducing Stochastic Normalizing Flows (SNF) - an arbitrary sequence of deterministic invertible functions and stochastic processes such as Markov Chain Monte Carlo (MCMC) or Langevin Dynamics. This combination can be powerful as adding stochasticity to a flow helps overcoming expressiveness limitations of a chosen deterministic invertible function, while the trainable flow transformations can improve the sampling efficiency over pure MCMC. Key to our approach is that we can match a marginal target density without having to marginalize out the stochasticity of traversed paths. Invoking ideas from nonequilibrium statistical mechanics, we introduce a training method that only uses conditional path probabilities. We can turn an SNF into a Boltzmann Generator that samples asymptotically unbiased from a given target density by importance sampling of these paths. We illustrate the representational power, sampling efficiency and asymptotic correctness of SNFs on several benchmarks.},
  archivePrefix = {arXiv},
  eprint = {2002.06707},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/IUAHQQ74/Wu et al. - 2020 - Stochastic Normalizing Flows.pdf},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  primaryClass = {physics, stat}
}

@online{yeUnsupervisedLearningDense2019,
  title = {Unsupervised {{Learning}} of {{Dense Optical Flow}}, {{Depth}} and {{Egomotion}} from {{Sparse Event Data}}},
  author = {Ye, Chengxi and Mitrokhin, Anton and Fermüller, Cornelia and Yorke, James A. and Aloimonos, Yiannis},
  date = {2019-02-25},
  url = {http://arxiv.org/abs/1809.08625},
  urldate = {2020-07-08},
  abstract = {In this work we present a lightweight, unsupervised learning pipeline for \textbackslash textit\{dense\} depth, optical flow and egomotion estimation from sparse event output of the Dynamic Vision Sensor (DVS). To tackle this low level vision task, we use a novel encoder-decoder neural network architecture - ECN. Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Due to the lightweight design, the inference part of the network runs at 250 FPS on a single GPU, making the pipeline ready for realtime robotics applications. Our experiments demonstrate significant improvements upon previous works that used deep learning on event data, as well as the ability of our pipeline to perform well during both day and night.},
  archivePrefix = {arXiv},
  eprint = {1809.08625},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/WQET2BYX/Ye et al. - 2019 - Unsupervised Learning of Dense Optical Flow, Depth.pdf;/home/jethro/Zotero/storage/P7BZCY7U/1809.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{you17_large_batch_train_convol_networ,
  title = {Large Batch Training of Convolutional Networks},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  date = {2017},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1708.03888v3},
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1708.03888},
  eprinttype = {arxiv},
  primaryClass = {cs.CV}
}

@article{zenkeRemarkableRobustnessSurrogate2020,
  title = {The Remarkable Robustness of Surrogate Gradient Learning for Instilling Complex Function in Spiking Neural Networks},
  author = {Zenke, Friedemann and Vogels, Tim P.},
  date = {2020-06-29},
  journaltitle = {bioRxiv},
  pages = {2020.06.29.176925},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.06.29.176925},
  url = {https://www.biorxiv.org/content/10.1101/2020.06.29.176925v1},
  urldate = {2020-07-16},
  abstract = {{$<$}p{$>$}Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. In comparison, the functional capabilities of models of spiking networks are still rudimentary. This shortcoming is mainly due to the lack of insight and practical algorithms to construct the necessary connectivity. Any such algorithm typically attempts to build networks by iteratively reducing the error compared to a desired output. But assigning credit to hidden units in multi-layered spiking networks has remained challenging due to the non-differentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity in spiking network models. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients impact learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative9s scale can substantially affect learning performance. When we combine surrogate gradients with a suitable activity regularization technique, robust information processing can be achieved in spiking networks even at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.{$<$}/p{$>$}},
  file = {/home/jethro/Zotero/storage/UTGUI82E/Zenke and Vogels - 2020 - The remarkable robustness of surrogate gradient le.pdf;/home/jethro/Zotero/storage/HIR8P23R/2020.06.29.html},
  langid = {english}
}

@article{zhu18_overv_machin_teach,
  title = {An Overview of Machine Teaching},
  author = {Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and Rafferty, Anna N.},
  date = {2018},
  journaltitle = {CoRR},
  url = {http://arxiv.org/abs/1801.05927v1},
  abstract = {In this paper we try to organize machine teaching as a coherent set of ideas. Each idea is presented as varying along a dimension. The collection of dimensions then form the problem space of machine teaching, such that existing teaching problems can be characterized in this space. We hope this organization allows us to gain deeper understanding of individual teaching problems, discover connections among them, and identify gaps in the field.},
  archivePrefix = {arXiv},
  eprint = {1801.05927},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/M3P8Z5DK/Zhu et al. - 2018 - An overview of machine teaching.pdf},
  primaryClass = {cs.LG}
}

@article{zhuEVFlowNetSelfSupervisedOptical2018,
  title = {{{EV}}-{{FlowNet}}: {{Self}}-{{Supervised Optical Flow Estimation}} for {{Event}}-Based {{Cameras}}},
  shorttitle = {{{EV}}-{{FlowNet}}},
  author = {Zhu, Alex Zihao and Yuan, Liangzhe and Chaney, Kenneth and Daniilidis, Kostas},
  date = {2018-06-26},
  journaltitle = {Robotics: Science and Systems XIV},
  doi = {10.15607/RSS.2018.XIV.062},
  url = {http://arxiv.org/abs/1802.06898},
  urldate = {2020-07-07},
  abstract = {Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.},
  archivePrefix = {arXiv},
  eprint = {1802.06898},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/X9P55F47/Zhu et al. - 2018 - EV-FlowNet Self-Supervised Optical Flow Estimatio.pdf;/home/jethro/Zotero/storage/78DTZMH8/1802.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  note = {Comment: 9 pages, 5 figures, 1 table. Accompanying video: https://youtu.be/eMHZBSoq0sE. Dataset: https://daniilidis-group.github.io/mvsec/, Robotics: Science and Systems 2018}
}

@online{zhuUnsupervisedEventbasedLearning2018,
  title = {Unsupervised {{Event}}-Based {{Learning}} of {{Optical Flow}}, {{Depth}}, and {{Egomotion}}},
  author = {Zhu, Alex Zihao and Yuan, Liangzhe and Chaney, Kenneth and Daniilidis, Kostas},
  date = {2018-12-19},
  url = {http://arxiv.org/abs/1812.08156},
  urldate = {2020-07-08},
  abstract = {In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.},
  archivePrefix = {arXiv},
  eprint = {1812.08156},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/ZSAK6TDK/Zhu et al. - 2018 - Unsupervised Event-based Learning of Optical Flow,.pdf;/home/jethro/Zotero/storage/6ZMPZP3J/1812.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 9 pages, 7 figures},
  primaryClass = {cs}
}

@inproceedings{ziebart2008_maxentrl,
  title = {Maximum Entropy Inverse Reinforcement Learning.},
  author = {Ziebart, Brian and Maas, Andrew and Bagnell, J. and Dey, Anind},
  date = {2008-01},
  pages = {1433--1438}
}


