:PROPERTIES:
:ID:       c6f55ad8-b2b5-4298-889a-80655ceeb650
:END:
#+title: A Distributional Code for Value in Dopamine-based Reinforcement Learning

- paper :: [[https://www.nature.com/articles/s41586-019-1924-6][https://www.nature.com/articles/s41586-019-1924-6]]
- related :: [[id:be63d7a1-322e-40df-a184-90ad2b8aabb4][Reinforcement Learning ⭐]]

* Abstract
Reward prediction errors are typically represented by a single scalar
quantity, as in temporal-difference learning. In distributional RL,
the reward prediction error consists of a diverse set of channels,
predicting multiple future outcomes. Different channels have different
relative scalings for positive and negative reward prediction errors.
Imbalances between these scalings cause each channel to learn a
different value prediction, and collectively represent a distribution
over possible rewards.

* Resources
- [[https://mtomassoli.github.io/2017/12/08/distributional_rl/][Distributional RL – Simple Machine Learning]]

