:PROPERTIES:
:ID:       03bfa7bf-2ab6-4360-80df-76e46fab2200
:END:
#+title: And the Bit Goes Down: Revisiting the Quantization of Neural Networks

- tags :: [[id:b96c5fa6-8300-479c-911b-3de4c397b1d5][Model Compression]]
- paper :: cite:stock19_and_bit_goes_down

This method minimizes the loss reconstruction error for in-domain
inputs, and does not require any labelled data.

[[file:images/model_compression/screenshot_2019-08-02_13-07-02.png]]

This method exploits the high correlation in the convolutions in
ResNet-like architectures by the use of product quantization (PQ). The
approach here focuses on reconstructing the activations, and not the
weights. This results in better in-domain reconstruction, and does not
require any supervision.

Vector Quantization (VQ) and Product Quantization (PQ) decompose the
high-dimensional space into a cartesian product of subspaces that are
quantized separately. These are typically studied under the context of
nearest neighbour search.

* 
bibliography:biblio.bib

