:PROPERTIES:
:ID:       5f98a234-3fce-41bd-a912-35f7ae7158eb
:END:
#+hugo_slug: byron_boots_perspectives_on_machine_learning_and_robotics
#+title: Byron Boots - Perspectives on Machine Learning and Robotics
#+roam_key: https://www.youtube.com/watch?v=lBcFMa83yC8

Robotics can be approached from 3 different perspectives.

#+DOWNLOADED: screenshot @ 2020-07-15 17:17:27
[[file:images/byron_boots_perspectives_on_machine_learning_and_robotics/screenshot2020-07-15_17-17-27_.png]]

* Machine Learning works because of structure

1. In function/policy class
2. In objective function
3. In datasets
4. In optimization

And robotics also has a lot of structure. How do we make choices for our ML
algorithms to exploit this structure?

* How should robots learn?
:PROPERTIES:
:ID:       9a6d9b02-1efe-487c-bba7-8cabe0dc556f
:END:

[[id:ff243a09-9980-4738-b638-0521cc2bbf42][Empirical Risk Minimization]] is too strong: data is not i.i.d, not batched, not
in the model class. Solution: use online learning, that makes minimal
assumptions.

Online learning has simple derivations, insights from optimization, and may
suggest better algorithms.

Model Predictive Control is an effective approach to handling challenging
dynamic environments with simple models, because it allows for local
optimization of simple policies.

[[id:be63d7a1-322e-40df-a184-90ad2b8aabb4][Reinforcement Learning]] can learn complex policies, and is often state-of-the-art
in simulated environments and games. However, the algorithms are data-hungry,
and the objective function is non-convex and difficult to optimize.

[[id:7ecd7d57-00d1-4a58-9061-105e1c324850][Imitation Learning]] provides a general way to accelerate policy learning, by
leveraging models/simulators and experts. We can leverage surrogate objective
functions that are easier to optimize.
