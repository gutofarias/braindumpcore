:PROPERTIES:
:ID:       a1321d06-600d-477e-ada5-b402020c9d03
:END:
#+hugo_slug: self_attention
#+title: Self-attention

The self-attention mechanism is a defining characteristic of Transformer models.
They can be viewed as a graph-like inductive bias that connects all tokens in a
sequence with a relevance-based pooling operation.
