:PROPERTIES:
:ID:       1c5fc68f-d928-4518-87ae-4fdd52ff5f8d
:END:
#+roam_key: https://distill.pub/2019/paths-perspective-on-value-learning/
#+hugo_slug: the_paths_perspective_on_value_learning
#+title: The Paths Perspective on Value Learning

- source :: https://distill.pub/2019/paths-perspective-on-value-learning/
- author :: [[id:33de8ffa-d5f8-4586-820c-4aa3aa0d5230][Sam Greydanus]]

[[id:6bcdf2f0-6f2b-47bf-95c1-180a1d81f497][Temporal Difference Learning]] can be viewed as expanding the agent's experience
by merging trajectories.

TD averages over more trajectories than Monte Carlo methods, because there are
never fewer simulated trajectories than real trajectories. This explains why TD
learning outperforms Monte Carlo in tabular environments.

However, with function approximators, Monte Carlo and TD can make bad value
updates. TD learning *amplifies these errors dramatically*. 

Great article explaining why TD learning can be beneficial, and when it can be
harmful.
