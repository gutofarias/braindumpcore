:PROPERTIES:
:ID:       87bab6c2-5346-4253-9e11-697c4e63d62a
:END:
#+title: Surrogate Gradient Learning In Spiking Neural Networks

- paper :: https://arxiv.org/pdf/1901.09948v2.pdf

[[id:54691107-81a0-4d7b-8c51-d8a74bde7f86][Spiking Neural Networks]] enable power-efficient network models, which
have become of increasing importance in embedded and auto-motive
applications. The power efficiency stems from dispensing of expensive
floating-point computations. 

Surrogate gradient methods overcome the difficulties associated with
the discontinuous non-linearity. Rather than changing the neuronal
model ([[id:81796d94-28b9-441f-aee2-3194e4d59434][Smoothed Spiking Neural Networks]]), surrogate gradients are
introduced to allow for numerical optimisation.

Surrogate gradients can also improve the memory access overhead of the
learning process. For example, a global loss can be replaced by a
number of local loss functions. Surrogate gradient methods also allow
for end-to-end training without specifying a coding scheme in the
hidden layers.

There are many different available surrogate functions used, and all
have reportedly some success
cite:neftci19_surrog_gradien_learn_spikin_neural_networ. All of the
functions used are non-linear and monotonically increasing towards the
firing threshold. This suggests that the details of the surrogate are
not crucial in ensuring success of the method.

bibliography:biblio.bib

